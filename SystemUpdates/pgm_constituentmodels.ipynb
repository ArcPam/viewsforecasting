{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4141fbca",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 constituent models \n",
    "\n",
    "## ViEWS production system, pgm level\n",
    "\n",
    "\n",
    "This notebook trains a set of regression models for use in the monthly updated ViEWS predicting fatalities ensemble\n",
    "\n",
    "The notebook does the following: \n",
    "1. Retrieves data through querysets and stores in DataSets, a list of dictionaries\n",
    "2. Specifies the metadata of a number of models, stores in ModelList, a list of dictionaries\n",
    "3. Trains the models in ModelList, stores the trained objects in model storage and prediction storage\n",
    "4. Saves part of ModelList as csv and the rest as pickles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9c55f",
   "metadata": {},
   "source": [
    "## Importing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c9b18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  |:---------------------------------|:-----------------------------------------------------------------------------------------|\r\n",
      "  | RETRY_FREQUENCY                  | 5                                                                                        |\r\n",
      "  | LOG_LEVEL                        | INFO                                                                                     |\r\n",
      "  | HANDSHAKE_PATH                   |                                                                                          |\r\n",
      "  | REPO_URL                         | https://www.github.com/prio-data/viewser                                                 |\r\n",
      "  | LATEST_KNOWN_VERSION             | 0.0.0                                                                                    |\r\n",
      "  | NOTEBOOK_SERVER_IMAGE_REPOSITORY | prio-data/viewserspace                                                                   |\r\n",
      "  | NOTEBOOK_SERVER_IMAGE_REGISTRY   | viewsregistry.azurecr.io                                                                 |\r\n",
      "  | ERROR_DUMP_DIRECTORY             | /Users/havardhegre1/.views/dumps                                                         |\r\n",
      "  | MODEL_OBJECT_SFTP_USER           | predictions                                                                              |\r\n",
      "  | MODEL_OBJECT_SFTP_PORT           | 22222                                                                                    |\r\n",
      "  | MODEL_OBJECT_SFTP_HOSTNAME       | hermes                                                                                   |\r\n",
      "  | MODEL_OBJECT_KEY_DB_HOSTNAME     | janus                                                                                    |\r\n",
      "  | MODEL_OBJECT_KEY_DB_DBNAME       | pred3_certs                                                                              |\r\n",
      "  | QUERYSET_MAX_RETRIES             | 500                                                                                      |\r\n",
      "  | QUERYSET_REMOTE_PATH             | querysets                                                                                |\r\n",
      "  | REMOTE_URL                       | http://views.pcr.uu.se/hermes                                                            |\r\n",
      "  | MODEL_METADATA_DATABASE_HOSTNAME | hermes                                                                                   |\r\n",
      "  | MODEL_METADATA_DATABASE_NAME     | forecasts3                                                                               |\r\n",
      "  | MODEL_METADATA_DATABASE_SCHEMA   | forecasts                                                                                |\r\n",
      "  | MODEL_METADATA_DATABASE_TABLE    | model                                                                                    |\r\n",
      "  | AZURE_BLOB_STORAGE_ACCOUNT_NAME  | viewsblobs                                                                               |\r\n",
      "  | AZURE_BLOB_STORAGE_ACCOUNT_KEY   | ilP95e+ci1mRVqFnqoKBRWzN8W99KWshw3YV9gup4MskVf4KiDZeFtcwtX9e5Ej0vsSF8DXOF+DBosTj8wgjwg== |\r\n",
      "  ---------------------------------------------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! viewser config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05e2f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f560a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFRegressor, XGBRFClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "from views_runs import storage\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "\n",
    "from views_forecasts.extensions import *\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "# Packages from Predicting Fatalies repository\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Tools')\n",
    "sys.path.append('../Intermediates')\n",
    "from FetchData import FetchData, RetrieveFromList, document_queryset, ReturnQsList, document_ensemble,data_integrity_check\n",
    "from ViewsEstimators import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ced9d3",
   "metadata": {},
   "source": [
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda79d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Mydropbox to /Users/havardhegre1/Dropbox (ViEWS)/ViEWS\n"
     ]
    }
   ],
   "source": [
    "# Common parameters:\n",
    "dev_id = 'Fatalities003'\n",
    "run_id = dev_id\n",
    "\n",
    "# Generating a new run if necessary\n",
    "\n",
    "#try:\n",
    "#    ViewsMetadata().new_run(name=run_id,description='pgm_level_fatalities',min_month=1,max_month=999)\n",
    "#except KeyError:\n",
    "#    if 'devel' not in run_id:\n",
    "#        warnings.warn('You are overwriting a production system')\n",
    "\n",
    "depvar=\"ln_ged_sb_dep\"\n",
    "\n",
    "RerunQuerysets = True\n",
    "        \n",
    "FutureStart = 518\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,3,6,12,36]\n",
    "#fi_steps = [1,3,6,12,36]\n",
    "\n",
    "# Specifying partitions\n",
    "calib_partitioner_dict = {\"train\":(121,408),\"predict\":(409,456)}\n",
    "test_partitioner_dict = {\"train\":(121,456),\"predict\":(457,504)}\n",
    "future_partitioner_dict = {\"train\":(121,504),\"predict\":(505,516)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = f'/Users/{os.getlogin()}/Dropbox (ViEWS)/ViEWS'\n",
    "print('Setting Mydropbox to',Mydropbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b6488",
   "metadata": {},
   "source": [
    "## Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7884d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .    \n",
      "A dataset with 8 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 19 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 29 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 24 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 23 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 30 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 8 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 11 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 36 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 31 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 42 columns, with data between t 1 and 852. (13110 units)\n",
      " .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o      \n",
      "A dataset with 20 columns, with data between t 1 and 852. (13110 units)\n",
      "Model:  fatalities002_pgm_baseline\n",
      "Model:  fatalities002_pgm_conflictlong\n",
      "Model:  fatalities002_pgm_escwa_drought\n",
      "Model:  fatalities002_pgm_natsoc\n",
      "Model:  fatalities002_pgm_broad\n",
      "Model:  fatalities002_pgm_conflict_history\n",
      "Model:  fatalities002_pgm_conflict_treelag\n",
      "Model:  fatalities002_pgm_conflict_sptime_dist\n",
      "Model:  fatalities003_acled\n",
      "Model:  fatalities003_protest_pr_dynamic_national\n",
      "Model:  fatalities003_protest_full_elect_econ_national\n",
      "Model:  fatalities002_pgm_country_level\n"
     ]
    }
   ],
   "source": [
    "# Create Markdown documentation of all querysets used\n",
    "level = 'pgm'\n",
    "qslist = ReturnQsList(level)\n",
    "document_queryset(qslist,dev_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63004977",
   "metadata": {},
   "outputs": [],
   "source": [
    "! viewser tables show acled2_pgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if RerunQuerysets:\n",
    "#    import pgm_querysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145fb2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .    baseline: A dataset with 8 columns, with data between t = 1 and 852; 13110 units.\n",
      " .    conflicttreelag: A dataset with 8 columns, with data between t = 1 and 852; 13110 units.\n",
      " .    broad: A dataset with 23 columns, with data between t = 1 and 852; 13110 units.\n",
      " .    conflictlong: A dataset with 19 columns, with data between t = 1 and 852; 13110 units.\n",
      " .    conflicthist: A dataset with 30 columns, with data between t = 1 and 852; 13110 units.\n",
      " .    natsoc: A dataset with 24 columns, with data between t = 1 and 852; 13110 units.\n",
      " .    conflictsptime_dist: A dataset with 11 columns, with data between t = 1 and 852; 13110 units.\n",
      " .    escwa_drought: A dataset with 29 columns, with data between t = 1 and 852; 13110 units.\n"
     ]
    }
   ],
   "source": [
    "from FetchData import fetch_pgm_data_from_model_def\n",
    "\n",
    "Datasets=fetch_pgm_data_from_model_def(qslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5faa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in Datasets:\n",
    "\n",
    "    data_integrity_check(ds,depvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a558c",
   "metadata": {},
   "source": [
    "# Generating predictions\n",
    "Using the ViEWS3 partitioning/stepshifting syntax. Training models for A: calibration partition and B: test partition, to test out some calibration routines. Most models trained with ln_ged_sb_best as outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63851566",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912abb1",
   "metadata": {},
   "source": [
    "# Specify models in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "ModelList = DefineEnsembleModels('pgm')\n",
    "    \n",
    "\n",
    "for imodel,model in enumerate(ModelList):\n",
    "    print(imodel, model['modelname'], model['data_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece718b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ensemble(ModelList,'sb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73418bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that checks whether the model exists, retrains if not, \n",
    "# and stores the predictions if they have not been stored before for this run.\n",
    "# To do: set the data_preprocessing to the function in the model dictionary\n",
    "\n",
    "level = 'pgm'\n",
    "includeFuture = False\n",
    "force_rewrite = True\n",
    "force_retrain = True\n",
    "store_remote = False\n",
    "\n",
    "from views_runs import Storage, StepshiftedModels\n",
    "from views_partitioning.data_partitioner import DataPartitioner\n",
    "from viewser import Queryset, Column\n",
    "from views_runs import operations\n",
    "from views_runs.run_result import RunResult\n",
    "\n",
    "i = 0\n",
    "for model in ModelList[10:]:\n",
    "    modelstore = storage.Storage()\n",
    "    ct = datetime.now()\n",
    "    print(i, model['modelname'])\n",
    "    print('Calibration partition', ct)\n",
    "    model['Algorithm_text'] = str(model['algorithm'])\n",
    "    model['RunResult_calib'] = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"calib\":calib_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"calib\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_calib',\n",
    "            author_name        = \"JED\",\n",
    "    )\n",
    "\n",
    "    model['predstore_calib'] = level +  '_' + model['modelname'] + '_calib'\n",
    "    ct = datetime.now()\n",
    "    if force_rewrite:\n",
    "        print(model['predstore_calib'], ', run',  run_id, 'force_rewrite=True, predicting')\n",
    "        predictions_calib = model['RunResult_calib'].run.predict(\"calib\",\"predict\", model['RunResult_calib'].data)\n",
    "\n",
    "        predictions_calib.to_parquet(model['predstore_calib']+'.parquet')\n",
    "        if store_remote:\n",
    "            predictions_calib.forecasts.set_run(run_id)\n",
    "            predictions_calib.forecasts.to_store(name=model['predstore_calib'],overwrite=True)\n",
    "    else:\n",
    "        print('Trying to retrieve predictions', ct)\n",
    "        try:\n",
    "            predictions_calib = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_calib'])\n",
    "        except KeyError:\n",
    "            print(model['predstore_calib'], ', run',  run_id, 'does not exist, predicting')\n",
    "            predictions_calib = model['RunResult_calib'].run.predict(\"calib\",\"predict\", model['RunResult_calib'].data)\n",
    "            predictions_calib.forecasts.set_run(run_id)\n",
    "            predictions_calib.forecasts.to_store(name=model['predstore_calib'])\n",
    "                \n",
    "    ct = datetime.now()\n",
    "    print('Test partition', ct)\n",
    "    modelstore = storage.Storage()\n",
    "    model['RunResult_test'] = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"test\":test_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"test\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_test',\n",
    "            author_name        = \"JED\",\n",
    "    )\n",
    "    ct = datetime.now()\n",
    "    if force_rewrite:\n",
    "        print(model['predstore_test'], ', run',  run_id, 'force_rewrite=True, predicting')\n",
    "        predictions_test = model['RunResult_test'].run.predict(\"test\",\"predict\", model['RunResult_test'].data)\n",
    "        \n",
    "        predictions_test.to_parquet(model['predstore_test']+'.parquet')\n",
    "        if store_remote:\n",
    "            predictions_test.forecasts.set_run(run_id)\n",
    "            predictions_test.forecasts.to_store(name=model['predstore_test'],overwrite=True)\n",
    "    else:\n",
    "        print('Trying to retrieve predictions', ct)\n",
    "    #    model['predstore_test'] = level +  '_' + model['modelname'] + '_test'\n",
    "        try:\n",
    "            predictions_test = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_test'])\n",
    "        except KeyError:\n",
    "            print(model['predstore_test'], ', run', run_id, 'does not exist, predicting')\n",
    "            predictions_test = model['RunResult_test'].run.predict(\"test\",\"predict\",model['RunResult_test'].data)\n",
    "            predictions_test.forecasts.set_run(run_id)\n",
    "            predictions_test.forecasts.to_store(name=model['predstore_test'])\n",
    "    # Predictions for true future\n",
    "    if includeFuture:\n",
    "        ct = datetime.now()\n",
    "        print('Future', ct)\n",
    "        modelstore = storage.Storage()\n",
    "        model['RunResult_future'] = RunResult.retrain_or_retrieve(\n",
    "                retrain            = force_retrain,\n",
    "                store              = modelstore,\n",
    "                partitioner        = DataPartitioner({\"test\":future_partitioner_dict}),\n",
    "                stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                queryset_name      = model['queryset'],\n",
    "                partition_name     = \"test\",\n",
    "                timespan_name      = \"train\",\n",
    "                storage_name       = model['modelname'] + '_future',\n",
    "                author_name        = \"JED\",\n",
    "        )\n",
    "        ct = datetime.now()\n",
    "        if force_rewrite:\n",
    "            print(model['predstore_future'], ', run',  run_id, 'force_rewrite=True, predicting')\n",
    "            predictions_future = model['RunResult_future'].run.predict(FutureStart, model['RunResult_future'].data)\n",
    "            predictions_future.to_parquet(model['predstore_future']+'.parquet')\n",
    "\n",
    "            if store_remote:\n",
    "                predictions_future.forecasts.set_run(run_id)\n",
    "                predictions_future.forecasts.to_store(name=model['predstore_future'],overwrite=True)\n",
    "        else:\n",
    "            print('Trying to retrieve predictions', ct)\n",
    "            model['predstore_future'] = level +  '_' + model['modelname'] + '_f' + str(FutureStart)\n",
    "            try:\n",
    "                predictions_future = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_future'])\n",
    "            except KeyError:\n",
    "                print(model['predstore_future'], ', run', run_id, 'does not exist, predicting')\n",
    "                predictions_future = model['RunResult_future'].run.future_point_predict(FutureStart,model['RunResult_future'].data)\n",
    "                predictions_future.forecasts.set_run(run_id)\n",
    "                predictions_future.forecasts.to_store(name=model['predstore_future'])  \n",
    "#    model['algorithm'] = []\n",
    "    i = i + 1\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0550b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMetaData = pd.DataFrame(ModelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51906edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMetaData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:viewser] *",
   "language": "python",
   "name": "conda-env-viewser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
