{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4141fbca",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 constituent models \n",
    "\n",
    "## ViEWS production system, pgm level\n",
    "\n",
    "\n",
    "This notebook trains a set of regression models for use in the monthly updated ViEWS predicting fatalities ensemble\n",
    "\n",
    "The notebook does the following: \n",
    "1. Retrieves data through querysets and stores in DataSets, a list of dictionaries\n",
    "2. Specifies the metadata of a number of models, stores in ModelList, a list of dictionaries\n",
    "3. Trains the models in ModelList, stores the trained objects in model storage and prediction storage\n",
    "4. Saves part of ModelList as csv and the rest as pickles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9c55f",
   "metadata": {},
   "source": [
    "## Importing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! viewser config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e2f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f560a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFRegressor, XGBRFClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "from views_runs import storage\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "\n",
    "from views_forecasts.extensions import *\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "# Packages from Predicting Fatalies repository\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Tools')\n",
    "sys.path.append('../Intermediates')\n",
    "from FetchData import FetchData, RetrieveFromList, document_queryset, ReturnQsList, document_ensemble,data_integrity_check\n",
    "from ViewsEstimators import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ced9d3",
   "metadata": {},
   "source": [
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda79d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Mydropbox to /Users/jim/Dropbox (ViEWS)/ViEWS\n"
     ]
    }
   ],
   "source": [
    "# Common parameters:\n",
    "dev_id = 'Fatalities002'\n",
    "run_id = dev_id\n",
    "\n",
    "# Generating a new run if necessary\n",
    "\n",
    "#try:\n",
    "#    ViewsMetadata().new_run(name=run_id,description='pgm_level_fatalities',min_month=1,max_month=999)\n",
    "#except KeyError:\n",
    "#    if 'devel' not in run_id:\n",
    "#        warnings.warn('You are overwriting a production system')\n",
    "\n",
    "depvar=\"ln_ged_sb_dep\"\n",
    "\n",
    "RerunQuerysets = True\n",
    "        \n",
    "FutureStart = 518\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,3,6,12,36]\n",
    "#fi_steps = [1,3,6,12,36]\n",
    "\n",
    "# Specifying partitions\n",
    "calib_partitioner_dict = {\"train\":(121,408),\"predict\":(409,456)}\n",
    "test_partitioner_dict = {\"train\":(121,456),\"predict\":(457,504)}\n",
    "future_partitioner_dict = {\"train\":(121,504),\"predict\":(505,516)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = f'/Users/{os.getlogin()}/Dropbox (ViEWS)/ViEWS'\n",
    "print('Setting Mydropbox to',Mydropbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b6488",
   "metadata": {},
   "source": [
    "## Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7884d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .    \n",
      "A dataset with 8 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 19 columns, with data between t 1 and 852. (13110 units)\n",
      " .    \n",
      "A dataset with 29 columns, with data between t 1 and 852. (13110 units)\n",
      " .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o   \n",
      "A dataset with 24 columns, with data between t 1 and 852. (13110 units)\n",
      " .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O \n",
      "A dataset with 23 columns, with data between t 1 and 852. (13110 units)\n",
      " .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O      O      o       .      o      O  \n",
      "  >> Error\n",
      "\n",
      "  priogrid_month/trf/missing.fill/_/trf/bool.gte/1/trf/temporal.decay/12/base/ged2_pgm.ged_sb_best_sum_nokgi/values returned http_status_code=500 message='Internal Server Error' posted_at=datetime.datetime(2023, 3, 6, 10, 29, 33, 8159) retries=0\n",
      "  priogrid_month/trf/missing.fill/_/trf/ops.ln/_/trf/spatial.lag/1_1_0_0/base/ged2_pgm.ged_sb_best_sum_nokgi/values returned http_status_code=500 message='Internal Server Error' posted_at=datetime.datetime(2023, 3, 6, 10, 29, 32, 200705) retries=0\n",
      "  priogrid_month/trf/missing.fill/_/trf/bool.gte/1/trf/temporal.decay/12/base/ged2_pgm.ged_ns_best_count_nokgi/values returned http_status_code=500 message='Internal Server Error' posted_at=datetime.datetime(2023, 3, 6, 10, 29, 32, 214584) retries=0\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  priogrid_month/trf/missing.fill/_/trf/temporal.tlag/8/trf/missing.fill/_/trf/ops.ln/_/base/ged2_pgm.ged_sb_best_sum_nokgi/values returned http_status_code=500 message='Internal Server Error' posted_at=datetime.datetime(2023, 3, 6, 10, 29, 32, 193441) retries=0\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create Markdown documentation of all querysets used\u001b[39;00m\n\u001b[1;32m      2\u001b[0m level \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpgm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m qslist \u001b[38;5;241m=\u001b[39m \u001b[43mReturnQsList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m document_queryset(qslist,dev_id)\n",
      "File \u001b[0;32m~/Work/ViEWS/ViEWS3/viewsforecasting/SystemUpdates/../Tools/FetchData.py:16\u001b[0m, in \u001b[0;36mReturnQsList\u001b[0;34m(level)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cm_querysets\u001b[38;5;241m.\u001b[39mget_cm_querysets()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m level \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpgm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpgm_querysets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pgm_querysets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognised level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Work/ViEWS/ViEWS3/viewsforecasting/SystemUpdates/../Tools/pgm_querysets.py:1059\u001b[0m, in \u001b[0;36mget_pgm_querysets\u001b[0;34m()\u001b[0m\n\u001b[1;32m    809\u001b[0m qs_conf_history \u001b[38;5;241m=\u001b[39m (Queryset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfatalities002_pgm_conflict_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpriogrid_month\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    810\u001b[0m \n\u001b[1;32m    811\u001b[0m                    \u001b[38;5;66;03m# target variable\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124m                             \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m   1055\u001b[0m                    )\n\u001b[1;32m   1057\u001b[0m data \u001b[38;5;241m=\u001b[39m qs_conf_history\u001b[38;5;241m.\u001b[39mpublish()\u001b[38;5;241m.\u001b[39mfetch()\n\u001b[0;32m-> 1059\u001b[0m \u001b[43mreport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# Conflict_treelag_d_1_d_2\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m qs_treelag \u001b[38;5;241m=\u001b[39m (Queryset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfatalities002_pgm_conflict_treelag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpriogrid_month\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m               \u001b[38;5;66;03m# target variable\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m               \u001b[38;5;241m.\u001b[39mwith_column(Column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mln_ged_sb_dep\u001b[39m\u001b[38;5;124m\"\u001b[39m, from_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mged2_pgm\u001b[39m\u001b[38;5;124m\"\u001b[39m, from_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mged_sb_best_sum_nokgi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                            )\n\u001b[1;32m   1104\u001b[0m               )\n",
      "File \u001b[0;32m~/Work/ViEWS/ViEWS3/viewsforecasting/SystemUpdates/../Tools/pgm_querysets.py:15\u001b[0m, in \u001b[0;36mreport\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreport\u001b[39m(df):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns, with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata between t \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_level_values(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_level_values(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_level_values(\u001b[38;5;241m1\u001b[39m)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m units)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m           )\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Create Markdown documentation of all querysets used\n",
    "level = 'pgm'\n",
    "qslist = ReturnQsList(level)\n",
    "document_queryset(qslist,dev_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if RerunQuerysets:\n",
    "#    import pgm_querysets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145fb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FetchData import fetch_pgm_data_from_model_def\n",
    "\n",
    "Datasets=fetch_pgm_data_from_model_def(qslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5faa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in Datasets:\n",
    "\n",
    "    data_integrity_check(ds,depvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a558c",
   "metadata": {},
   "source": [
    "# Generating predictions\n",
    "Using the ViEWS3 partitioning/stepshifting syntax. Training models for A: calibration partition and B: test partition, to test out some calibration routines. Most models trained with ln_ged_sb_best as outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63851566",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912abb1",
   "metadata": {},
   "source": [
    "# Specify models in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "ModelList = DefineEnsembleModels('pgm')\n",
    "    \n",
    "\n",
    "for imodel,model in enumerate(ModelList):\n",
    "    print(imodel, model['modelname'], model['data_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece718b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ensemble(ModelList,'sb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73418bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop that checks whether the model exists, retrains if not, \n",
    "# and stores the predictions if they have not been stored before for this run.\n",
    "# To do: set the data_preprocessing to the function in the model dictionary\n",
    "\n",
    "level = 'pgm'\n",
    "includeFuture = True\n",
    "\n",
    "from views_runs import Storage, StepshiftedModels\n",
    "from views_partitioning.data_partitioner import DataPartitioner\n",
    "from viewser import Queryset, Column\n",
    "from views_runs import operations\n",
    "from views_runs.run_result import RunResult\n",
    "\n",
    "i = 0\n",
    "for model in ModelList[:]:\n",
    "    force_retrain = True\n",
    "    modelstore = storage.Storage()\n",
    "    ct = datetime.now()\n",
    "    print(i, model['modelname'])\n",
    "    print('Calibration partition', ct)\n",
    "    model['Algorithm_text'] = str(model['algorithm'])\n",
    "    model['RunResult_calib'] = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"calib\":calib_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"calib\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_calib',\n",
    "            author_name        = \"JED\",\n",
    "    )\n",
    "\n",
    "    model['predstore_calib'] = level +  '_' + model['modelname'] + '_calib'\n",
    "    ct = datetime.now()\n",
    "    print('Trying to retrieve predictions', ct)\n",
    "    try:\n",
    "        predictions_calib = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_calib'])\n",
    "    except KeyError:\n",
    "        print(model['predstore_calib'], ', run',  run_id, 'does not exist, predicting')\n",
    "        predictions_calib = model['RunResult_calib'].run.predict(\"calib\",\"predict\", model['RunResult_calib'].data)\n",
    "        predictions_calib.forecasts.set_run(run_id)\n",
    "        predictions_calib.forecasts.to_store(name=model['predstore_calib'])\n",
    "\n",
    "    ct = datetime.now()\n",
    "    print('Test partition', ct)\n",
    "    modelstore = storage.Storage()\n",
    "    model['RunResult_test'] = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"test\":test_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"test\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_test',\n",
    "            author_name        = \"JED\",\n",
    "    )\n",
    "    ct = datetime.now()\n",
    "    print('Trying to retrieve predictions', ct)\n",
    "    model['predstore_test'] = level +  '_' + model['modelname'] + '_test'\n",
    "    try:\n",
    "        predictions_test = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_test'])\n",
    "    except KeyError:\n",
    "        print(model['predstore_test'], ', run', run_id, 'does not exist, predicting')\n",
    "        predictions_test = model['RunResult_test'].run.predict(\"test\",\"predict\",model['RunResult_test'].data)\n",
    "        predictions_test.forecasts.set_run(run_id)\n",
    "        predictions_test.forecasts.to_store(name=model['predstore_test'])\n",
    "    # Predictions for true future\n",
    "    if includeFuture:\n",
    "        ct = datetime.now()\n",
    "        print('Future', ct)\n",
    "        modelstore = storage.Storage()\n",
    "        model['RunResult_future'] = RunResult.retrain_or_retrieve(\n",
    "                retrain            = force_retrain,\n",
    "                store              = modelstore,\n",
    "                partitioner        = DataPartitioner({\"test\":future_partitioner_dict}),\n",
    "                stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                queryset_name      = model['queryset'],\n",
    "                partition_name     = \"test\",\n",
    "                timespan_name      = \"train\",\n",
    "                storage_name       = model['modelname'] + '_future',\n",
    "                author_name        = \"JED\",\n",
    "        )\n",
    "        ct = datetime.now()\n",
    "        print('Trying to retrieve predictions', ct)\n",
    "        model['predstore_future'] = level +  '_' + model['modelname'] + '_f' + str(FutureStart)\n",
    "        try:\n",
    "            predictions_future = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_future'])\n",
    "        except KeyError:\n",
    "            print(model['predstore_future'], ', run', run_id, 'does not exist, predicting')\n",
    "            predictions_future = model['RunResult_future'].run.future_point_predict(FutureStart,model['RunResult_future'].data)\n",
    "            predictions_future.forecasts.set_run(run_id)\n",
    "            predictions_future.forecasts.to_store(name=model['predstore_future'])  \n",
    "#    model['algorithm'] = []\n",
    "    i = i + 1\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0550b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMetaData = pd.DataFrame(ModelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51906edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelMetaData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:viewser] *",
   "language": "python",
   "name": "conda-env-viewser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
