{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1098f7cb",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 constituent models \n",
    "## ViEWS-ESCWA model production system, cm level\n",
    "\n",
    "\n",
    "This notebook trains a set of regression models for use in the monthly updated ViEWS-ESCWA predicting  ensemble\n",
    "\n",
    "The notebook does the following: \n",
    "1. Retrieves data through querysets and stores in DataSets, a list of dictionaries\n",
    "2. Specifies the metadata of a number of models, stores in ModelList, a list of dictionaries\n",
    "3. Trains the models in ModelList, stores the trained objects in model storage and prediction storage\n",
    "4. Saves part of ModelList as csv and the rest as pickles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f7cba",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8855fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef27dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFRegressor, XGBRFClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "from views_runs import storage\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "\n",
    "from views_forecasts.extensions import *\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "# Packages from viewsforecasting repository\n",
    "\n",
    "#from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Tools')\n",
    "sys.path.append('../Intermediates')\n",
    "from FetchData import FetchData, RetrieveFromList, document_queryset, ReturnQsList, document_ensemble\n",
    "from ViewsEstimators import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300ea25",
   "metadata": {},
   "source": [
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ae8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters:\n",
    "dev_id = 'escwa001'\n",
    "run_id = dev_id\n",
    "\n",
    "# Generating a new run if necessary\n",
    "\n",
    "#try:\n",
    "#    ViewsMetadata().new_run(name=run_id,description='Developing the fatalities model for FCDO',min_month=1,max_month=999)\n",
    "#except KeyError:\n",
    "#    if 'devel' not in run_id:\n",
    "#        warnings.warn('You are overwriting a production system')\n",
    "\n",
    "RerunQuerysets = True\n",
    "\n",
    "EndOfHistory = 517\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,3,6,12,36]\n",
    "#fi_steps = [1,3,6,12,36]\n",
    "\n",
    "# Specifying partitions\n",
    "calib_partitioner_dict = {\"train\":(121,408),\"predict\":(409,456)}\n",
    "test_partitioner_dict = {\"train\":(121,456),\"predict\":(457,504)}\n",
    "future_partitioner_dict = {\"train\":(121,504),\"predict\":(505,516)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = f'/Users/{os.getlogin()}/Dropbox (ViEWS)/ViEWS'\n",
    "# Intitilaprint('Setting Mydropbox to',Mydropbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f2f350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>min_month</th>\n",
       "      <th>max_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tests</td>\n",
       "      <td>Just for testing/development</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>devel</td>\n",
       "      <td>Development Runs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fat_2022_2</td>\n",
       "      <td>Fatalities run for February 2022</td>\n",
       "      <td>505.0</td>\n",
       "      <td>542.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>escwa_2022_2</td>\n",
       "      <td>ESCWA run for February 2022</td>\n",
       "      <td>505.0</td>\n",
       "      <td>542.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jpr_2022_2</td>\n",
       "      <td>JPR run for February 2022</td>\n",
       "      <td>505.0</td>\n",
       "      <td>542.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fat_devel_v1</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test_run_x1</td>\n",
       "      <td>Bug fixes and other problems</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fat_devel_v2</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lags</td>\n",
       "      <td>Developing lag models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fat_cm_storage_test</td>\n",
       "      <td>testing storage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fat_lags_no_ds</td>\n",
       "      <td>Developing lag models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fat_lags_ds_002</td>\n",
       "      <td>Developing lag models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>fat_lags_downssampled_002</td>\n",
       "      <td>Developing lag models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fat_lags_downssampled_01</td>\n",
       "      <td>Developing lag models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fat_lags_not_downssampled</td>\n",
       "      <td>Developing lag models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>climate_extremes</td>\n",
       "      <td>Developing climate extreme models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fat_devel_v3</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fat_devel_v4</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fat_lags_model_devel</td>\n",
       "      <td>Developing lag models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>fat_devel_uncertainty</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>fat_devel_v5</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>fat_devel_v6</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>fat_devel_v7</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>prod_devel_v1</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>clim_pred</td>\n",
       "      <td>Developing climate predictions models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>prod_devel_v2</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>prod_devel_v4</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>small_test</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>fatalities_march2022_alm</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>fatal_devel_v8</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>fatal_devel_v7_</td>\n",
       "      <td>Fatal_pgm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>fatal_devel_v7</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>fcdo_pgm</td>\n",
       "      <td>Developing lag models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>fat_devel_pgm_v7</td>\n",
       "      <td>pgm_level_fatalities</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>fatalities001</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>fatalities002</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>devel_protest</td>\n",
       "      <td>Maxine development protest paper</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>fatalities002_cw</td>\n",
       "      <td>Script testing for Chandler</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>fatalities002_mr</td>\n",
       "      <td>Fatalities models done for mapping</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>fat_dev_mc_media</td>\n",
       "      <td>Developing the fatalities model for FCDO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>fat_short</td>\n",
       "      <td>Short fatality model</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Fat_short</td>\n",
       "      <td>Short fatality model</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>devel_escalation001</td>\n",
       "      <td>pgm_level_fatalities</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>clim_pred_dev</td>\n",
       "      <td>Developing climate predictions models</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>fatalities003</td>\n",
       "      <td>cm_fatalities003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>escwa001</td>\n",
       "      <td>Respecified VIEWS-ESCWA model</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name                               description  \\\n",
       "id                                                                        \n",
       "1                       tests              Just for testing/development   \n",
       "2                       devel                          Development Runs   \n",
       "3                  fat_2022_2          Fatalities run for February 2022   \n",
       "4                escwa_2022_2               ESCWA run for February 2022   \n",
       "5                  jpr_2022_2                 JPR run for February 2022   \n",
       "16               fat_devel_v1  Developing the fatalities model for FCDO   \n",
       "17                test_run_x1              Bug fixes and other problems   \n",
       "18               fat_devel_v2  Developing the fatalities model for FCDO   \n",
       "19                       lags                     Developing lag models   \n",
       "20        fat_cm_storage_test                           testing storage   \n",
       "21             fat_lags_no_ds                     Developing lag models   \n",
       "22            fat_lags_ds_002                     Developing lag models   \n",
       "23  fat_lags_downssampled_002                     Developing lag models   \n",
       "24   fat_lags_downssampled_01                     Developing lag models   \n",
       "25  fat_lags_not_downssampled                     Developing lag models   \n",
       "26           climate_extremes         Developing climate extreme models   \n",
       "27               fat_devel_v3  Developing the fatalities model for FCDO   \n",
       "28               fat_devel_v4  Developing the fatalities model for FCDO   \n",
       "29       fat_lags_model_devel                     Developing lag models   \n",
       "30      fat_devel_uncertainty  Developing the fatalities model for FCDO   \n",
       "31               fat_devel_v5  Developing the fatalities model for FCDO   \n",
       "32               fat_devel_v6  Developing the fatalities model for FCDO   \n",
       "33               fat_devel_v7  Developing the fatalities model for FCDO   \n",
       "34              prod_devel_v1  Developing the fatalities model for FCDO   \n",
       "35                  clim_pred     Developing climate predictions models   \n",
       "36              prod_devel_v2  Developing the fatalities model for FCDO   \n",
       "37              prod_devel_v4  Developing the fatalities model for FCDO   \n",
       "38                 small_test  Developing the fatalities model for FCDO   \n",
       "39   fatalities_march2022_alm  Developing the fatalities model for FCDO   \n",
       "40             fatal_devel_v8  Developing the fatalities model for FCDO   \n",
       "41            fatal_devel_v7_                                 Fatal_pgm   \n",
       "42             fatal_devel_v7  Developing the fatalities model for FCDO   \n",
       "43                   fcdo_pgm                     Developing lag models   \n",
       "44           fat_devel_pgm_v7                      pgm_level_fatalities   \n",
       "45              fatalities001  Developing the fatalities model for FCDO   \n",
       "46              fatalities002  Developing the fatalities model for FCDO   \n",
       "48              devel_protest          Maxine development protest paper   \n",
       "49           fatalities002_cw               Script testing for Chandler   \n",
       "50           fatalities002_mr        Fatalities models done for mapping   \n",
       "51           fat_dev_mc_media  Developing the fatalities model for FCDO   \n",
       "52                  fat_short                      Short fatality model   \n",
       "53                  Fat_short                      Short fatality model   \n",
       "54        devel_escalation001                      pgm_level_fatalities   \n",
       "55              clim_pred_dev     Developing climate predictions models   \n",
       "56              fatalities003                          cm_fatalities003   \n",
       "57                   escwa001             Respecified VIEWS-ESCWA model   \n",
       "\n",
       "    min_month  max_month  \n",
       "id                        \n",
       "1         NaN        NaN  \n",
       "2         NaN        NaN  \n",
       "3       505.0      542.0  \n",
       "4       505.0      542.0  \n",
       "5       505.0      542.0  \n",
       "16        1.0      999.0  \n",
       "17        1.0      999.0  \n",
       "18        1.0      999.0  \n",
       "19        1.0      999.0  \n",
       "20        1.0      999.0  \n",
       "21        1.0      999.0  \n",
       "22        1.0      999.0  \n",
       "23        1.0      999.0  \n",
       "24        1.0      999.0  \n",
       "25        1.0      999.0  \n",
       "26        1.0      999.0  \n",
       "27        1.0      999.0  \n",
       "28        1.0      999.0  \n",
       "29        1.0      999.0  \n",
       "30        1.0      999.0  \n",
       "31        1.0      999.0  \n",
       "32        1.0      999.0  \n",
       "33        1.0      999.0  \n",
       "34        1.0      999.0  \n",
       "35        1.0      999.0  \n",
       "36        1.0      999.0  \n",
       "37        1.0      999.0  \n",
       "38        1.0      999.0  \n",
       "39        1.0      999.0  \n",
       "40        1.0      999.0  \n",
       "41        1.0      999.0  \n",
       "42        1.0      999.0  \n",
       "43        1.0      999.0  \n",
       "44        1.0      999.0  \n",
       "45        1.0      999.0  \n",
       "46        1.0      999.0  \n",
       "48        1.0      999.0  \n",
       "49        1.0      999.0  \n",
       "50        1.0      999.0  \n",
       "51        1.0      999.0  \n",
       "52        1.0      999.0  \n",
       "53        1.0      999.0  \n",
       "54        1.0      999.0  \n",
       "55        1.0      999.0  \n",
       "56        1.0      999.0  \n",
       "57        1.0      999.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViewsMetadata().get_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430d5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the run:\n",
    "initialize_run = False\n",
    "if initialize_run:\n",
    "    ViewsMetadata().new_run(name='escwa001', description='Respecified VIEWS-ESCWA model',min_month=1,max_month=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf0208",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a457b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .     .    escwa001_vdem A dataset with 52 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_baseline; A dataset with 6 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_topics_stub; A dataset with 64 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_aquastat_stub; A dataset with 11 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_cm_conflict_history_stub; A dataset with 24 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_cm_conflict_history_ext; A dataset with 33 columns, with data between t = 1 and 852. (213 units)\n",
      " .    fatalities002_vdem_short_stub; A dataset with 58 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_wdi_short_stub; A dataset with 28 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_joint_narrow; A dataset with 41 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_joint_broad_stub; A dataset with 77 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_faostat_stub;A dataset with 35 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_faoprices_stub;A dataset with 11 columns, with data between t 1 and 852. (213 units)\n",
      " .    fatalities002_imfweo_stub;A dataset with 5 columns, with data between t 1 and 852. (213 units)\n",
      " .     .     .     .     .     .     .     .     .     .    Model:  escwa001_cflong\n",
      "Model:  escwa001_vdem\n"
     ]
    }
   ],
   "source": [
    "# Create Markdown documentation of all querysets used\n",
    "level = 'cm'\n",
    "qslist = ReturnQsList(level)\n",
    "document_queryset(qslist,dev_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e75122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .    vdem001: A dataset with 52 columns, with data between t = 1 and 852; 213 units.\n",
      " .    cflong001: A dataset with 44 columns, with data between t = 1 and 852; 213 units.\n"
     ]
    }
   ],
   "source": [
    "from FetchData import fetch_cm_data_from_model_def\n",
    "\n",
    "Datasets=fetch_cm_data_from_model_def(qslist, EndOfHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925bdb3",
   "metadata": {},
   "source": [
    "# Generating predictions\n",
    "Using the ViEWS3 partitioning/stepshifting syntax. Training models for A: calibration partition and B: test partition, to test out some calibration routines. Most models trained with ln_ged_sb_best as outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990574dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in Datasets:\n",
    "    if 'topics' in ds['Name']:\n",
    "        print(ds['df'].columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf49bd2",
   "metadata": {},
   "source": [
    "## Checking missingness and infinity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe61e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vdem001\n",
      "ged_sb_dep 158230 missing: 0 infinity: 0\n",
      "ged_sb_dummy_t0 158230 missing: 0 infinity: 0\n",
      "vdem_v2x_delibdem 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_egaldem 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_libdem 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_libdem_48 158230 missing: 15166 infinity: 0\n",
      "vdem_v2x_partip 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_partipdem 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_accountability 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_civlib 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_clphy 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_cspart 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_divparctrl 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_edcomp_thick 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_egal 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_execorr 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_frassoc_thick 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_gencs 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_gender 158230 missing: 15391 infinity: 0\n",
      "vdem_v2x_genpp 158230 missing: 15391 infinity: 0\n",
      "vdem_v2x_horacc 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_neopat 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_pubcorr 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_rule 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_veracc 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_ex_military 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_ex_party 158230 missing: 15145 infinity: 0\n",
      "vdem_v2x_freexp 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xcl_acjst 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xcl_dmove 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xcl_prpty 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xcl_rol 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xcl_slave 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xdd_dd 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xdl_delib 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xeg_eqdr 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xeg_eqprotec 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xel_frefair 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xel_regelec 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xme_altinf 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xnp_client 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xnp_regcorr 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xpe_exlecon 158230 missing: 15400 infinity: 0\n",
      "vdem_v2xpe_exlpol 158230 missing: 15400 infinity: 0\n",
      "vdem_v2xpe_exlgeo 158230 missing: 15400 infinity: 0\n",
      "vdem_v2xpe_exlgender 158230 missing: 15400 infinity: 0\n",
      "vdem_v2xpe_exlsocgr 158230 missing: 15400 infinity: 0\n",
      "vdem_v2xps_party 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xcs_ccsi 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xnp_pres 158230 missing: 15145 infinity: 0\n",
      "vdem_v2xeg_eqaccess 158230 missing: 15145 infinity: 0\n",
      "cflong001\n",
      "ged_sb_dep 158230 missing: 0 infinity: 0\n",
      "ged_sb_dummy_t0 158230 missing: 0 infinity: 0\n",
      "ged_os_dummy_t0 158230 missing: 0 infinity: 0\n",
      "ged_ns_dummy_t0 158230 missing: 0 infinity: 0\n",
      "ged_sb_dummy_t1 158230 missing: 5 infinity: 0\n",
      "ged_sb_dummy_t2 158230 missing: 5 infinity: 0\n",
      "ged_sb_dummy_t3 158230 missing: 5 infinity: 0\n",
      "_ged_sb_dummy_t3 158230 missing: 5 infinity: 0\n",
      "ged_sb_dummy_t4 158230 missing: 5 infinity: 0\n",
      "ged_sb_dummy_t5 158230 missing: 5 infinity: 0\n",
      "ged_sb_dummy_t6 158230 missing: 11 infinity: 0\n",
      "ged_os_dummy_t1 158230 missing: 5 infinity: 0\n",
      "ged_os_dummy_t2 158230 missing: 5 infinity: 0\n",
      "ged_os_dummy_t3 158230 missing: 5 infinity: 0\n",
      "_ged_os_dummy_t3 158230 missing: 5 infinity: 0\n",
      "ged_os_dummy_t4 158230 missing: 5 infinity: 0\n",
      "ged_os_dummy_t5 158230 missing: 5 infinity: 0\n",
      "ged_os_dummy_t6 158230 missing: 11 infinity: 0\n",
      "ged_ns_dummy_t1 158230 missing: 5 infinity: 0\n",
      "ged_ns_dummy_t2 158230 missing: 5 infinity: 0\n",
      "ged_ns_dummy_t3 158230 missing: 5 infinity: 0\n",
      "_ged_ns_dummy_t3 158230 missing: 5 infinity: 0\n",
      "ged_ns_dummy_t4 158230 missing: 5 infinity: 0\n",
      "ged_ns_dummy_t5 158230 missing: 5 infinity: 0\n",
      "ged_ns_dummy_t6 158230 missing: 11 infinity: 0\n",
      "decay_ged_sb_1 158230 missing: 0 infinity: 0\n",
      "decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "decay_ged_sb_25 158230 missing: 0 infinity: 0\n",
      "decay_ged_sb_100 158230 missing: 0 infinity: 0\n",
      "decay_ged_sb_500 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_1 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_5 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_25 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_100 158230 missing: 0 infinity: 0\n",
      "decay_ged_ns_1 158230 missing: 0 infinity: 0\n",
      "decay_ged_ns_5 158230 missing: 0 infinity: 0\n",
      "decay_ged_ns_25 158230 missing: 0 infinity: 0\n",
      "decay_ged_ns_100 158230 missing: 0 infinity: 0\n",
      "splag_1_decay_ged_sb_25 158230 missing: 0 infinity: 0\n",
      "splag_1_decay_ged_sb_500 158230 missing: 0 infinity: 0\n",
      "splag_1_decay_ged_os_25 158230 missing: 0 infinity: 0\n",
      "splag_1_decay_ged_os_500 158230 missing: 0 infinity: 0\n",
      "splag_1_decay_ged_ns_25 158230 missing: 0 infinity: 0\n",
      "splag_1_decay_ged_ns_500 158230 missing: 0 infinity: 0\n"
     ]
    }
   ],
   "source": [
    "N=51\n",
    "for i in range(len(Datasets)):\n",
    "    df = Datasets[i]['df']\n",
    "    print(Datasets[i]['Name'])\n",
    "    for col in df.iloc[: , :N].columns:\n",
    "        print(col,len(df[col]), 'missing:', df[col].isnull().sum(), 'infinity:', np.isinf(df).values.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761eb9c",
   "metadata": {},
   "source": [
    "# Specify models in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425514d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 escwa001_cflong cflong001\n",
      "1 escwa001_vdem vdem001\n"
     ]
    }
   ],
   "source": [
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "ModelList = DefineEnsembleModels('cm')\n",
    "    \n",
    "for imodel,model in enumerate(ModelList):\n",
    "    print(imodel, model['modelname'], model['data_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b1b6322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelname': 'escwa001_cflong',\n",
       "  'algorithm': RandomForestClassifier(n_estimators=300, n_jobs=12),\n",
       "  'depvar': 'ged_sb_dep',\n",
       "  'data_train': 'cflong001',\n",
       "  'queryset': 'escwa001_cflong',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Long conflict history model',\n",
       "  'long_description': '...',\n",
       "  'predstore_calib': 'cm_escwa001_cflong_calib',\n",
       "  'predstore_test': 'cm_escwa001_cflong_test'},\n",
       " {'modelname': 'escwa001_vdem',\n",
       "  'algorithm': RandomForestClassifier(n_estimators=300, n_jobs=12),\n",
       "  'depvar': 'ged_sb_dep',\n",
       "  'data_train': 'vdem001',\n",
       "  'queryset': 'escwa001_vdem',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Varieties of democracy model',\n",
       "  'long_description': '...',\n",
       "  'predstore_calib': 'cm_escwa001_vdem_calib',\n",
       "  'predstore_test': 'cm_escwa001_vdem_test'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613b4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 escwa001_cflong cflong001\n",
      "1 escwa001_vdem vdem001\n"
     ]
    }
   ],
   "source": [
    "document_ensemble(ModelList,'sb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4d636ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vdem001 ged_sb_dep                     0\n",
      "ged_sb_dummy_t0                0\n",
      "vdem_v2x_delibdem          15145\n",
      "vdem_v2x_egaldem           15145\n",
      "vdem_v2x_libdem            15145\n",
      "vdem_v2x_libdem_48         15166\n",
      "vdem_v2x_partip            15145\n",
      "vdem_v2x_partipdem         15145\n",
      "vdem_v2x_accountability    15145\n",
      "vdem_v2x_civlib            15145\n",
      "vdem_v2x_clphy             15145\n",
      "vdem_v2x_cspart            15145\n",
      "vdem_v2x_divparctrl        15145\n",
      "vdem_v2x_edcomp_thick      15145\n",
      "vdem_v2x_egal              15145\n",
      "vdem_v2x_execorr           15145\n",
      "vdem_v2x_frassoc_thick     15145\n",
      "vdem_v2x_gencs             15145\n",
      "vdem_v2x_gender            15391\n",
      "vdem_v2x_genpp             15391\n",
      "vdem_v2x_horacc            15145\n",
      "vdem_v2x_neopat            15145\n",
      "vdem_v2x_pubcorr           15145\n",
      "vdem_v2x_rule              15145\n",
      "vdem_v2x_veracc            15145\n",
      "vdem_v2x_ex_military       15145\n",
      "vdem_v2x_ex_party          15145\n",
      "vdem_v2x_freexp            15145\n",
      "vdem_v2xcl_acjst           15145\n",
      "vdem_v2xcl_dmove           15145\n",
      "vdem_v2xcl_prpty           15145\n",
      "vdem_v2xcl_rol             15145\n",
      "vdem_v2xcl_slave           15145\n",
      "vdem_v2xdd_dd              15145\n",
      "vdem_v2xdl_delib           15145\n",
      "vdem_v2xeg_eqdr            15145\n",
      "vdem_v2xeg_eqprotec        15145\n",
      "vdem_v2xel_frefair         15145\n",
      "vdem_v2xel_regelec         15145\n",
      "vdem_v2xme_altinf          15145\n",
      "vdem_v2xnp_client          15145\n",
      "vdem_v2xnp_regcorr         15145\n",
      "vdem_v2xpe_exlecon         15400\n",
      "vdem_v2xpe_exlpol          15400\n",
      "vdem_v2xpe_exlgeo          15400\n",
      "vdem_v2xpe_exlgender       15400\n",
      "vdem_v2xpe_exlsocgr        15400\n",
      "vdem_v2xps_party           15145\n",
      "vdem_v2xcs_ccsi            15145\n",
      "vdem_v2xnp_pres            15145\n",
      "vdem_v2xeg_eqaccess        15145\n",
      "vdem_v2x_diagacc           15145\n",
      "dtype: int64\n",
      "cflong001 ged_sb_dep                   0\n",
      "ged_sb_dummy_t0              0\n",
      "ged_os_dummy_t0              0\n",
      "ged_ns_dummy_t0              0\n",
      "ged_sb_dummy_t1              5\n",
      "ged_sb_dummy_t2              5\n",
      "ged_sb_dummy_t3              5\n",
      "_ged_sb_dummy_t3             5\n",
      "ged_sb_dummy_t4              5\n",
      "ged_sb_dummy_t5              5\n",
      "ged_sb_dummy_t6             11\n",
      "ged_os_dummy_t1              5\n",
      "ged_os_dummy_t2              5\n",
      "ged_os_dummy_t3              5\n",
      "_ged_os_dummy_t3             5\n",
      "ged_os_dummy_t4              5\n",
      "ged_os_dummy_t5              5\n",
      "ged_os_dummy_t6             11\n",
      "ged_ns_dummy_t1              5\n",
      "ged_ns_dummy_t2              5\n",
      "ged_ns_dummy_t3              5\n",
      "_ged_ns_dummy_t3             5\n",
      "ged_ns_dummy_t4              5\n",
      "ged_ns_dummy_t5              5\n",
      "ged_ns_dummy_t6             11\n",
      "decay_ged_sb_1               0\n",
      "decay_ged_sb_5               0\n",
      "decay_ged_sb_25              0\n",
      "decay_ged_sb_100             0\n",
      "decay_ged_sb_500             0\n",
      "decay_ged_os_1               0\n",
      "decay_ged_os_5               0\n",
      "decay_ged_os_25              0\n",
      "decay_ged_os_100             0\n",
      "decay_ged_ns_1               0\n",
      "decay_ged_ns_5               0\n",
      "decay_ged_ns_25              0\n",
      "decay_ged_ns_100             0\n",
      "splag_1_decay_ged_sb_25      0\n",
      "splag_1_decay_ged_sb_500     0\n",
      "splag_1_decay_ged_os_25      0\n",
      "splag_1_decay_ged_os_500     0\n",
      "splag_1_decay_ged_ns_25      0\n",
      "splag_1_decay_ged_ns_500     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for ds in Datasets:\n",
    "    df = ds['df']\n",
    "    print(ds['Name'],df.isna().sum())\n",
    "    ds['df']=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10c58f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 escwa001_cflong\n",
      "Calibration partition 2023-05-03 21:23:21.447161\n",
      " * == Performing a run: \"escwa001_cflong_calib\" == * \n",
      "Model object named \"escwa001_cflong_calib\" with equivalent metadata already exists.\n",
      "Retrain is true, overwriting \"escwa001_cflong_calib\"\n",
      "Training model(s)...\n",
      "Storing \"escwa001_cflong_calib\"\n",
      "cm_escwa001_cflong_calib , run escwa001 force_rewrite=True, predicting\n",
      "Test partition 2023-05-03 21:25:32.414794\n",
      " * == Performing a run: \"escwa001_cflong_test\" == * \n",
      "Model object named \"escwa001_cflong_test\" with equivalent metadata already exists.\n",
      "Retrain is true, overwriting \"escwa001_cflong_test\"\n",
      "Training model(s)...\n",
      "Storing \"escwa001_cflong_test\"\n",
      "cm_escwa001_cflong_test , run escwa001 force_rewrite=True, predicting\n",
      "1 escwa001_vdem\n",
      "Calibration partition 2023-05-03 21:28:13.658211\n",
      " * == Performing a run: \"escwa001_vdem_calib\" == * \n",
      "Training model(s)...\n",
      "Storing \"escwa001_vdem_calib\"\n",
      "cm_escwa001_vdem_calib , run escwa001 force_rewrite=True, predicting\n",
      "Test partition 2023-05-03 21:30:00.382105\n",
      " * == Performing a run: \"escwa001_vdem_test\" == * \n",
      "Training model(s)...\n",
      "Storing \"escwa001_vdem_test\"\n",
      "cm_escwa001_vdem_test , run escwa001 force_rewrite=True, predicting\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "# Loop that checks whether the model exists, retrains if not, \n",
    "# and stores the predictions if they have not been stored before for this run.\n",
    "# To do: set the data_preprocessing to the function in the model dictionary\n",
    "\n",
    "level = 'cm'\n",
    "includeFuture = False\n",
    "force_rewrite = True\n",
    "force_retrain = True\n",
    "\n",
    "from views_runs import Storage, StepshiftedModels\n",
    "from views_partitioning.data_partitioner import DataPartitioner\n",
    "from viewser import Queryset, Column\n",
    "from views_runs import operations\n",
    "from views_runs.run_result import RunResult\n",
    "from new_markov import markov\n",
    "\n",
    "i = 0\n",
    "for model in ModelList[:]:\n",
    "    if 'Markov' not in model['modelname']:\n",
    "        \n",
    "        modelstore = storage.Storage()\n",
    "        ct = datetime.now()\n",
    "        print(i, model['modelname'])\n",
    "        print('Calibration partition', ct)\n",
    "        model['Algorithm_text'] = str(model['algorithm'])\n",
    "        model['RunResult_calib'] = RunResult.retrain_or_retrieve(\n",
    "                retrain            = force_retrain,\n",
    "                store              = modelstore,\n",
    "                partitioner        = DataPartitioner({\"calib\":calib_partitioner_dict}),\n",
    "                stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                queryset_name      = model['queryset'],\n",
    "                partition_name     = \"calib\",\n",
    "                timespan_name      = \"train\",\n",
    "                storage_name       = model['modelname'] + '_calib',\n",
    "                author_name        = \"JED\",\n",
    "        )\n",
    "\n",
    "    #    model['predstore_calib'] = level +  '_' + model['modelname'] + '_calib'\n",
    "        ct = datetime.now()\n",
    "        if force_rewrite:\n",
    "            print(model['predstore_calib'], ', run',  run_id, 'force_rewrite=True, predicting')\n",
    "            predictions_calib = model['RunResult_calib'].run.predict(\"calib\",\"predict\", model['RunResult_calib'].data)\n",
    "            predictions_calib.forecasts.set_run(run_id)\n",
    "            predictions_calib.forecasts.to_store(name=model['predstore_calib'],overwrite=True)\n",
    "        else:\n",
    "            print('Trying to retrieve predictions', ct)\n",
    "            try:\n",
    "                predictions_calib = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_calib'])\n",
    "            except KeyError:\n",
    "                print(model['predstore_calib'], ', run',  run_id, 'does not exist, predicting')\n",
    "                predictions_calib = model['RunResult_calib'].run.predict(\"calib\",\"predict\", model['RunResult_calib'].data)\n",
    "                predictions_calib.forecasts.set_run(run_id)\n",
    "                predictions_calib.forecasts.to_store(name=model['predstore_calib'])\n",
    "\n",
    "        ct = datetime.now()\n",
    "        print('Test partition', ct)\n",
    "        modelstore = storage.Storage()\n",
    "        model['RunResult_test'] = RunResult.retrain_or_retrieve(\n",
    "                retrain            = force_retrain,\n",
    "                store              = modelstore,\n",
    "                partitioner        = DataPartitioner({\"test\":test_partitioner_dict}),\n",
    "                stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                queryset_name      = model['queryset'],\n",
    "                partition_name     = \"test\",\n",
    "                timespan_name      = \"train\",\n",
    "                storage_name       = model['modelname'] + '_test',\n",
    "                author_name        = \"JED\",\n",
    "        )\n",
    "        ct = datetime.now()\n",
    "        \n",
    "        if force_rewrite:\n",
    "            print(model['predstore_test'], ', run',  run_id, 'force_rewrite=True, predicting')\n",
    "            predictions_test = model['RunResult_test'].run.predict(\"test\",\"predict\", model['RunResult_test'].data)\n",
    "            predictions_test.forecasts.set_run(run_id)\n",
    "            predictions_test.forecasts.to_store(name=model['predstore_test'],overwrite=True)\n",
    "        else:\n",
    "            print('Trying to retrieve predictions', ct)\n",
    "    #    model['predstore_test'] = level +  '_' + model['modelname'] + '_test'\n",
    "            try:\n",
    "                predictions_test = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_test'])\n",
    "            except KeyError:\n",
    "                print(model['predstore_test'], ', run', run_id, 'does not exist, predicting')\n",
    "                predictions_test = model['RunResult_test'].run.predict(\"test\",\"predict\",model['RunResult_test'].data)\n",
    "                predictions_test.forecasts.set_run(run_id)\n",
    "                predictions_test.forecasts.to_store(name=model['predstore_test'])\n",
    "        # Predictions for true future\n",
    "        if includeFuture:\n",
    "            ct = datetime.now()\n",
    "            print('Future', ct)\n",
    "            modelstore = storage.Storage()\n",
    "            model['RunResult_future'] = RunResult.retrain_or_retrieve(\n",
    "                    retrain            = force_retrain,\n",
    "                    store              = modelstore,\n",
    "                    partitioner        = DataPartitioner({\"test\":future_partitioner_dict}),\n",
    "                    stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                    dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                    queryset_name      = model['queryset'],\n",
    "                    partition_name     = \"test\",\n",
    "                    timespan_name      = \"train\",\n",
    "                    storage_name       = model['modelname'] + '_future',\n",
    "                    author_name        = \"JED\",\n",
    "            )\n",
    "            ct = datetime.now()\n",
    "            \n",
    "            if force_rewrite:\n",
    "                print(model['predstore_future'], ', run',  run_id, 'force_rewrite=True, predicting')\n",
    "                predictions_future = model['RunResult_future'].run.predict(EndOfHistory, model['RunResult_future'].data)\n",
    "                predictions_future.forecasts.set_run(run_id)\n",
    "                predictions_future.forecasts.to_store(name=model['predstore_future'],overwrite=True)\n",
    "            else:\n",
    "                print('Trying to retrieve predictions', ct)\n",
    "                model['predstore_future'] = level +  '_' + model['modelname'] + '_f' + str(EndOfHistory)\n",
    "                predictions_future.forecasts.to_store(name=model['predstore_future'])  \n",
    "                \n",
    "    else:\n",
    "        modelstore = storage.Storage()\n",
    "        ct = datetime.now()\n",
    "        print(i, model['modelname'])\n",
    "        print('Calibration partition', ct)\n",
    "        model['Algorithm_text'] = str(model['algorithm'])\n",
    "        print('Trying to retrieve predictions', ct)\n",
    "        if force_retrain:\n",
    "            print(model['predstore_calib'], ', run',  run_id, 'force_retrain = True, predicting')\n",
    "            predictions_calib = markov.compute_markov(calib_partitioner_dict, EndOfHistory, model['depvar'], 'calib', model['algorithm'])\n",
    "            predictions_calib.forecasts.set_run(run_id)\n",
    "            predictions_calib.forecasts.to_store(name=model['predstore_calib'],overwrite=True)\n",
    "        else:\n",
    "            try:\n",
    "                predictions_calib = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_calib'])\n",
    "            except KeyError:\n",
    "                print(model['predstore_calib'], ', run',  run_id, 'does not exist, predicting')\n",
    "                predictions_calib = markov.compute_markov(calib_partitioner_dict, EndOfHistory, model['depvar'], 'calib', model['algorithm'])\n",
    "                predictions_calib.forecasts.set_run(run_id)\n",
    "                predictions_calib.forecasts.to_store(name=model['predstore_calib'],overwrite=True)\n",
    "                \n",
    "        ct = datetime.now()\n",
    "        print('Test partition', ct)\n",
    "        modelstore = storage.Storage()\n",
    "        if force_retrain:\n",
    "            print(model['predstore_test'], ', run', run_id, 'force_retrain=True, predicting')\n",
    "            predictions_test = markov.compute_markov(test_partitioner_dict, EndOfHistory, model['depvar'], 'test', model['algorithm'])\n",
    "            predictions_test.forecasts.set_run(run_id)\n",
    "            predictions_test.forecasts.to_store(name=model['predstore_test'],overwrite=True)\n",
    "        else:\n",
    "            try:\n",
    "                predictions_test = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_test'])\n",
    "            except KeyError:\n",
    "                print(model['predstore_test'], ', run', run_id, 'does not exist, predicting')\n",
    "                predictions_test = markov.compute_markov(test_partitioner_dict, EndOfHistory, model['depvar'], 'test', model['algorithm'])\n",
    "                predictions_test.forecasts.set_run(run_id)\n",
    "                predictions_test.forecasts.to_store(name=model['predstore_test'],overwrite=True)\n",
    "                \n",
    "        if includeFuture:\n",
    "            ct = datetime.now()\n",
    "            print('Future', ct)\n",
    "            modelstore = storage.Storage()\n",
    "            print('Trying to retrieve predictions', ct)\n",
    "            model['predstore_future'] = level +  '_' + model['modelname'] + '_f' + str(EndOfHistory)\n",
    "            if force_retrain:\n",
    "                print(model['predstore_future'], ', run', run_id, 'force_retrain=True, predicting')\n",
    "                predictions_future = markov.compute_markov(test_partitioner_dict, EndOfHistory, model['depvar'], 'future', model['algorithm'])\n",
    "                predictions_future.forecasts.set_run(run_id)\n",
    "                predictions_future.forecasts.to_store(name=model['predstore_future'],overwrite=True)\n",
    "            else:\n",
    "                try:\n",
    "                    predictions_future = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_future'])\n",
    "                except KeyError:\n",
    "                    print(model['predstore_future'], ', run', run_id, 'does not exist, predicting')\n",
    "                    predictions_future = markov.compute_markov(test_partitioner_dict, EndOfHistory, model['depvar'], 'future', model['algorithm'])\n",
    "                    predictions_future.forecasts.set_run(run_id)\n",
    "                    predictions_future.forecasts.to_store(name=model['predstore_future'],overwrite=True)  \n",
    "                            \n",
    "        print('**************************************************************')\n",
    "    i = i + 1\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b52249",
   "metadata": {},
   "source": [
    "## Notes on training time for the various algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are calculated in minutes for the hh20 feature set (with about 40 features), for all 36 steps, calibration (c) and test (t) partitions, also include generating predictions, and are approximate:\n",
    "\n",
    "#nj=12 (number of threads)\n",
    "#scikit random forest:        21:13 (c), 26:20 (t) RandomForestRegressor(n_estimators=200, n_jobs=nj)\n",
    "#XGB random forest:           06:02 (c), 07:51 (t) XGBRFRegressor(n_estimators=300,n_jobs=nj)\n",
    "#scikit gbm:                  13:59 (c), 15:55 (t) GradientBoostingRegressor(), \n",
    "#scikit hurdle random forest: 07:32 (c), 09:49 (t) For both clf and reg: (n_estimators=200, n_jobs=nj)\n",
    "#XGB hurdle xgb:              01:26 (c), 01:32 (t) For both clf and reg:                n_estimators=200,tree_method='hist',n_jobs=nj)\n",
    "#scikit histgbm:              01:17 (c), 01:20 (t) HistGradientBoostingRegressor(max_iter=200)\n",
    "#XGB xgb:                     01:00 (c), 01:04 (t) XGBRegressor(n_estimators=200,tree_method='hist',n_jobs=nj)\n",
    "#lightgbm gbm:                00:25 (c), --    (t) LGBMRegressor(n_estimators=100,num_threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71483a35",
   "metadata": {},
   "source": [
    "# Various helper functions and tools...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7570c6",
   "metadata": {},
   "source": [
    "# Retrieving external forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30211fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve David's Markov models\n",
    "## To do: rewrite the model dictionary to the new, slimmer version.\n",
    "#DRList = []\n",
    "\n",
    "\n",
    "#model = {\n",
    "#    'modelname':   'fatalities002_Markov_glm',\n",
    "#    'algorithm': [],\n",
    "#    'depvar': \"ln_ged_sb_dep\",\n",
    "#    'data_train':      'joint_narrow',\n",
    "#    'queryset': 'fatalities002_joint_narrow',\n",
    "#}\n",
    "#DRList.append(model)\n",
    "\n",
    "#model = {\n",
    "#    'modelname':   'fatalities002_Markov_rf',\n",
    "#    'algorithm': [],\n",
    "#    'depvar': \"ln_ged_sb_dep\",\n",
    "#    'data_train':      'joint_narrow',\n",
    "#    'queryset': 'fatalities002_joint_narrow',\n",
    "#}\n",
    "\n",
    "#DRList.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = f'/Users/{os.getlogin()}/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "\n",
    "#DRList[0]['predictions_file_calib'] = path + 'markov_jointnarrow_ss_glm_calib.parquet'\n",
    "#DRList[0]['predictions_file_test'] = path + 'markov_jointnarrow_ss_glm_test.parquet'\n",
    "#DRList[0]['predictions_file_future'] = path + 'vmm_glm_hh20_517.csv'\n",
    "\n",
    "#DRList[1]['predictions_file_calib'] = path + 'markov_jointnarrow_ss_rf_calib.parquet'\n",
    "#DRList[1]['predictions_file_test'] = path + 'markov_jointnarrow_ss_rf_test.parquet'\n",
    "#DRList[1]['predictions_file_future'] = path + 'vmm_rf_hh20_517.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86478962",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing Markov models in central storage\n",
    "## Retrieving dependent variable\n",
    "\n",
    "#print('Adding depvar - CHECK FILES BEING USED FROM STORAGE ARE SUITABLE!')\n",
    "#target_calib = pd.DataFrame.forecasts.read_store('cm_fatalities002_conflicthistory_rf_calib', run=run_id)['ln_ged_sb_dep']\n",
    "#target_test = pd.DataFrame.forecasts.read_store('cm_fatalities002_conflicthistory_rf_test', run=run_id)['ln_ged_sb_dep']\n",
    "#level = 'cm'\n",
    "#for model in DRList:\n",
    "#    df_calib = pd.read_parquet(model['predictions_file_calib'])\n",
    "##    df_calib.rename(columns={'target_month_id':'month_id'}, inplace=True)\n",
    "##    df_calib.set_index(['month_id', 'country_id'], inplace=True)\n",
    "\n",
    "#    df_test = pd.read_parquet(model['predictions_file_test'])\n",
    "##    df_test.rename(columns={'target_month_id':'month_id'}, inplace=True)\n",
    "##    df_calib.set_index(['month_id', 'country_id'], inplace=True)\n",
    "\n",
    "##    df_future = pd.read_csv(model['predictions_file_future'],index_col=['month_id','country_id'])\n",
    "#    df_calib['ln_ged_sb_dep'] = target_calib\n",
    "#    df_test['ln_ged_sb_dep'] = target_test\n",
    "##    df_future['ln_ged_sb_dep'] = np.nan # Empty dependent variable column for consistency/required by prediction storage function\n",
    "#    stored_modelname = level + '_' + model['modelname'] + '_calib'\n",
    "#    df_calib.forecasts.set_run(run_id)\n",
    "#    df_calib.forecasts.to_store(name=stored_modelname, overwrite=True)\n",
    "#    stored_modelname = level + '_' + model['modelname'] + '_test'\n",
    "#    df_test.forecasts.set_run(run_id)\n",
    "#    df_test.forecasts.to_store(name=stored_modelname, overwrite=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907a4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:viewser] *",
   "language": "python",
   "name": "conda-env-viewser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
