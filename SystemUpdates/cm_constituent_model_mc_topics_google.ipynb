{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1098f7cb",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 constituent models \n",
    "## ViEWS production system, cm level\n",
    "\n",
    "\n",
    "This notebook trains a set of regression models for use in the monthly updated ViEWS predicting fatalities ensemble\n",
    "\n",
    "The notebook does the following: \n",
    "1. Retrieves data through querysets and stores in DataSets, a list of dictionaries\n",
    "2. Specifies the metadata of a number of models, stores in ModelList, a list of dictionaries\n",
    "3. Trains the models in ModelList, stores the trained objects in model storage and prediction storage\n",
    "4. Saves part of ModelList as csv and the rest as pickles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f7cba",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8855fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef27dd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline queryset pushed forward from the fatalities002, new updated version.\n",
      " Topics has been slightly changed to reflect the new time lag. Note in the future we should be able to tlag by 1 only as for example, by September 7th we should have all data for October. So technically ahead of the ucdp update schedule. For right now, though, we have data through July so given that we are running the model in October I lagged the value by 3 instead of 1. I converted the 1 year lag to 15 months, and then replaced the running average by 3 month lag with stock over 12. Note that the running average is not the exact version as is calculated by Rauh and Muller team but a stand in. Additional variables will be added.\n",
      " Google trends has similarly been lagged to reflect the last available data is August. There might be something wrong with my querysets where the NAs are getting autofilled with 0s when querying. So, I am setting the google_index value by tlag 2. \n",
      " Internet usage is available at country-year level, but through 2020 only. So lag by 2 years or 24 months.\n",
      " I have borrowed the population from features where t12 lag wdi_cy version has been used.\n",
      " Something interesting to note, queryset for google trends does not seem to have NAs inserted, but NAs appear in the datagrip. there does not seem to be an automatic fill zeros so I am not sure what caused the queryset on this data to insert the zeroes. Could not test and figure out more reasons. I have additionally made to sure to use the missing fill on all the querysets.\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRFRegressor, XGBRFClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "#import views_dataviz\n",
    "from views_runs import storage\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "\n",
    "from views_forecasts.extensions import *\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "# Packages from viewsforecasting repository\n",
    "\n",
    "#from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Tools')\n",
    "sys.path.append('../Intermediates')\n",
    "from FetchData import FetchData, RetrieveFromList, document_queryset, ReturnQsList, document_ensemble\n",
    "from ViewsEstimators import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300ea25",
   "metadata": {},
   "source": [
    "## Common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c76adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda list | grep views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fdc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# find out why and where missingness occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ae8aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Mydropbox to /Users/malika/Dropbox (ViEWS)/ViEWS\n"
     ]
    }
   ],
   "source": [
    "# Common parameters:\n",
    "dev_id = 'fat_dev_mc_media'\n",
    "run_id = dev_id\n",
    "\n",
    "# Generating a new run if necessary\n",
    "\n",
    "#try:\n",
    " #  ViewsMetadata().new_run(name=run_id,description='Developing the fatalities model for FCDO',min_month=1,max_month=999)\n",
    "#except KeyError:\n",
    "  #if 'devel' not in run_id:\n",
    "     #   warnings.warn('You are overwriting a production system')\n",
    "\n",
    "RerunQuerysets = True\n",
    "\n",
    "FutureStart = 515\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,3,6,12,36]\n",
    "#fi_steps = [1,3,6,12,36]\n",
    "\n",
    "# Specifying partitions\n",
    "calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,444)}\n",
    "test_partitioner_dict = {\"train\":(121,444),\"predict\":(445,492)}\n",
    "future_partitioner_dict = {\"train\":(121,492),\"predict\":(493,504)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = f'/Users/{os.getlogin()}/Dropbox (ViEWS)/ViEWS'\n",
    "print('Setting Mydropbox to',Mydropbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf0208",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a457b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .      o   fat_dev_mc_media_baseline; A dataset with 6 columns, with data between t 1 and 852. (213 units)\n",
      " .      o   fat_dev_mc_media_topics_stub; A dataset with 60 columns, with data between t 1 and 852. (213 units)\n",
      " .      o   fat_dev_mc_media_google_internet_stub; A dataset with 13 columns, with data between t 1 and 852. (213 units)\n",
      " .     .    Model:  fat_dev_mc_media_baseline\n",
      "Model:  fat_dev_mc_media_topics\n",
      "Model:  fat_dev_mc_media_google_internet\n",
      "Model:  fat_dev_mc_media_all_features\n"
     ]
    }
   ],
   "source": [
    "# Create Markdown documentation of all querysets used\n",
    "level = 'cm'\n",
    "qslist = ReturnQsList(level)\n",
    "document_queryset(qslist,dev_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e75122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. qss 4\n",
      "defined ['fat_dev_mc_media_baseline', 'fat_dev_mc_media_topics', 'fat_dev_mc_media_google_internet', 'fat_dev_mc_media_all_features']\n",
      "model ['fat_dev_mc_media_baseline', 'fat_dev_mc_media_topics', 'fat_dev_mc_media_all_features', 'fat_dev_mc_media_google_internet']\n",
      " .    media_baseline: A dataset with 6 columns, with data between t = 1 and 852; 213 units.\n",
      " .    media_topics: A dataset with 66 columns, with data between t = 1 and 852; 213 units.\n",
      " .    media_all_features: A dataset with 79 columns, with data between t = 1 and 852; 213 units.\n",
      " .    media_google_internet: A dataset with 19 columns, with data between t = 1 and 852; 213 units.\n",
      "len datasets 4\n"
     ]
    }
   ],
   "source": [
    "from FetchData import fetch_cm_data_from_model_def\n",
    "\n",
    "Datasets=fetch_cm_data_from_model_def(qslist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925bdb3",
   "metadata": {},
   "source": [
    "# Generating predictions\n",
    "Using the ViEWS3 partitioning/stepshifting syntax. Training models for A: calibration partition and B: test partition, to test out some calibration routines. Most models trained with ln_ged_sb_best as outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6424da7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fat_dev_mc_media'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b379f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ln_ged_sb_dep</th>\n",
       "      <th>ln_ged_sb</th>\n",
       "      <th>wdi_sp_pop_totl</th>\n",
       "      <th>decay_ged_sb_5</th>\n",
       "      <th>decay_ged_os_5</th>\n",
       "      <th>splag_1_decay_ged_sb_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_id</th>\n",
       "      <th>country_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>359531.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1084744.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15182611.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155525.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ln_ged_sb_dep  ln_ged_sb  wdi_sp_pop_totl  \\\n",
       "month_id country_id                                              \n",
       "1        1                     0.0        0.0         780153.0   \n",
       "         2                     0.0        0.0         359531.0   \n",
       "         3                     0.0        0.0        1084744.0   \n",
       "         4                     0.0        0.0       15182611.0   \n",
       "         5                     0.0        0.0         155525.0   \n",
       "\n",
       "                     decay_ged_sb_5  decay_ged_os_5  splag_1_decay_ged_sb_5  \n",
       "month_id country_id                                                          \n",
       "1        1                      0.0             0.0                     0.0  \n",
       "         2                      0.0             0.0                     0.0  \n",
       "         3                      0.0             0.0                     0.0  \n",
       "         4                      0.0             0.0                     0.0  \n",
       "         5                      0.0             0.0                     0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Datasets[0]['df'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990574dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ModelMetadata in module views_schema.models:\n",
      "\n",
      "class ModelMetadata(pydantic.main.BaseModel)\n",
      " |  ModelMetadata(*, author: str, queryset_name: str, train_start: int, train_end: int, steps: Optional[List[int]] = None, training_date: datetime.datetime) -> None\n",
      " |  \n",
      " |  ModelMetadata\n",
      " |  =============\n",
      " |  \n",
      " |  Data used to organize model objects.\n",
      " |  \n",
      " |  parameters:\n",
      " |      author (str): Name of the user that authored the model object.\n",
      " |      queryset_name (str): Name of the queryset used to train the model\n",
      " |      train_start (int): Month identifier for training start date\n",
      " |      train_start (int): Month identifier for training end date\n",
      " |      training_date (datetime.datetime): Timestamp for training date (use datetime.datetime.now())\n",
      " |  \n",
      " |  example:\n",
      " |  \n",
      " |      # Instantiate the class with values\n",
      " |  \n",
      " |      my_metadata = ModelMetadata(\n",
      " |          author = \"my_name\",\n",
      " |          queryset_name = \"my_queryset\",\n",
      " |          train_start = 1,\n",
      " |          train_end = 300,\n",
      " |          steps = [1,2,3],\n",
      " |          training_date = datetime.datetime.now())\n",
      " |  \n",
      " |      # Create metadata with a views_runs.ViewsRun object. This fetches\n",
      " |      # values from the associated StepshiftedModels and DataPartitioner\n",
      " |      # objects.\n",
      " |  \n",
      " |      my_metadata = my_run.create_model_metadata(\n",
      " |              author = \"me\",\n",
      " |              queryset_name = \"my_queryset\",\n",
      " |              training_partition_name = \"A\",\n",
      " |              )\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ModelMetadata\n",
      " |      pydantic.main.BaseModel\n",
      " |      pydantic.utils.Representation\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'author': <class 'str'>, 'queryset_name': <class 's...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'views_schema.models.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = None\n",
      " |  \n",
      " |  __fields__ = {'author': ModelField(name='author', type=str, required=T...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __post_root_validators__ = []\n",
      " |  \n",
      " |  __pre_root_validators__ = []\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, author: str, queryset_name: str, ... No...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __repr_args__(self) -> 'ReprArgs'\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> 'unicode'\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: 'unicode' = None, encoding: 'unicode' = 'utf8', proto: pydantic.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}') -> 'DictStrAny' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: 'unicode' = '#/definitions/{model}', **dumps_kwargs: Any) -> 'unicode' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  Config = <class 'pydantic.config.BaseConfig'>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> 'unicode'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'unicode'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'unicode') -> 'unicode'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> 'unicode'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from views_runs import ModelMetadata \n",
    "help(ModelMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf49bd2",
   "metadata": {},
   "source": [
    "## Checking missingness and infinity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe61e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "media_baseline\n",
      "ln_ged_sb_dep 158230 missing: 0 infinity: 0\n",
      "ln_ged_sb 158230 missing: 0 infinity: 0\n",
      "wdi_sp_pop_totl 158230 missing: 11 infinity: 0\n",
      "decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_5 158230 missing: 0 infinity: 0\n",
      "splag_1_decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "media_topics\n",
      "ln_ged_sb_dep 158230 missing: 0 infinity: 0\n",
      "ln_ged_sb 158230 missing: 0 infinity: 0\n",
      "wdi_sp_pop_totl 158230 missing: 11 infinity: 0\n",
      "topic0_religion_t3 158230 missing: 5 infinity: 0\n",
      "topic0_religion_t15 158230 missing: 11 infinity: 0\n",
      "topic1_politics_t3 158230 missing: 5 infinity: 0\n",
      "topic1_politics_t15 158230 missing: 11 infinity: 0\n",
      "topic2_sanctions_3 158230 missing: 5 infinity: 0\n",
      "topic2_sanctions_t15 158230 missing: 11 infinity: 0\n",
      "topic3_life_t3 158230 missing: 5 infinity: 0\n",
      "topic3_life_t15 158230 missing: 11 infinity: 0\n",
      "topic4_energy_t3 158230 missing: 5 infinity: 0\n",
      "topic4_energy_t15 158230 missing: 11 infinity: 0\n",
      "topic5_media_t3 158230 missing: 5 infinity: 0\n",
      "topic5_media_t15 158230 missing: 11 infinity: 0\n",
      "topic6_economics_t3 158230 missing: 5 infinity: 0\n",
      "topic6_economics_t15 158230 missing: 11 infinity: 0\n",
      "topic7_health_t3 158230 missing: 5 infinity: 0\n",
      "topic7_health_t15 158230 missing: 11 infinity: 0\n",
      "topic8_china_t3 158230 missing: 5 infinity: 0\n",
      "topic8_china_t15 158230 missing: 11 infinity: 0\n",
      "topic9_foreign_t3 158230 missing: 5 infinity: 0\n",
      "topic9_foreign_t15 158230 missing: 11 infinity: 0\n",
      "topic10_conflict_t3 158230 missing: 5 infinity: 0\n",
      "topic10_conflict_t15 158230 missing: 11 infinity: 0\n",
      "topic11_diplomacy_t3 158230 missing: 5 infinity: 0\n",
      "topic11_diplomacy_t15 158230 missing: 11 infinity: 0\n",
      "topic12_power_t3 158230 missing: 5 infinity: 0\n",
      "topic12_power_t15 158230 missing: 11 infinity: 0\n",
      "topic13_sports_t3 158230 missing: 5 infinity: 0\n",
      "topic13_sports_t15 158230 missing: 11 infinity: 0\n",
      "topic14_judiciary_t3 158230 missing: 5 infinity: 0\n",
      "topic14_judiciary_t15 158230 missing: 11 infinity: 0\n",
      "decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_5 158230 missing: 0 infinity: 0\n",
      "topic0_religion_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic1_politics_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic2_sanctions_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic3_life_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic4_energy_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic5_media_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic6_economics_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic7_health_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic8_china_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic9_foreign_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic10_conflict_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic11_diplomacy_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic12_power_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic13_sports_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic14_judiciary_t3_stock 158230 missing: 5 infinity: 0\n",
      "splag_1_decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "media_all_features\n",
      "ln_ged_sb_dep 158230 missing: 0 infinity: 0\n",
      "ln_ged_sb 158230 missing: 0 infinity: 0\n",
      "wdi_sp_pop_totl 158230 missing: 11 infinity: 0\n",
      "_wdi_sp_pop_totl 158230 missing: 11 infinity: 0\n",
      "topic0_religion_t3 158230 missing: 5 infinity: 0\n",
      "topic0_religion_t15 158230 missing: 11 infinity: 0\n",
      "topic1_politics_t3 158230 missing: 5 infinity: 0\n",
      "topic1_politics_t15 158230 missing: 11 infinity: 0\n",
      "topic2_sanctions_3 158230 missing: 5 infinity: 0\n",
      "topic2_sanctions_t15 158230 missing: 11 infinity: 0\n",
      "topic3_life_t3 158230 missing: 5 infinity: 0\n",
      "topic3_life_t15 158230 missing: 11 infinity: 0\n",
      "topic4_energy_t3 158230 missing: 5 infinity: 0\n",
      "topic4_energy_t15 158230 missing: 11 infinity: 0\n",
      "topic5_media_t3 158230 missing: 5 infinity: 0\n",
      "topic5_media_t15 158230 missing: 11 infinity: 0\n",
      "topic6_economics_t3 158230 missing: 5 infinity: 0\n",
      "topic6_economics_t15 158230 missing: 11 infinity: 0\n",
      "topic7_health_t3 158230 missing: 5 infinity: 0\n",
      "topic7_health_t15 158230 missing: 11 infinity: 0\n",
      "topic8_china_t3 158230 missing: 5 infinity: 0\n",
      "topic8_china_t15 158230 missing: 11 infinity: 0\n",
      "topic9_foreign_t3 158230 missing: 5 infinity: 0\n",
      "topic9_foreign_t15 158230 missing: 11 infinity: 0\n",
      "topic10_conflict_t3 158230 missing: 5 infinity: 0\n",
      "topic10_conflict_t15 158230 missing: 11 infinity: 0\n",
      "topic11_diplomacy_t3 158230 missing: 5 infinity: 0\n",
      "topic11_diplomacy_t15 158230 missing: 11 infinity: 0\n",
      "topic12_power_t3 158230 missing: 5 infinity: 0\n",
      "topic12_power_t15 158230 missing: 11 infinity: 0\n",
      "topic13_sports_t3 158230 missing: 5 infinity: 0\n",
      "topic13_sports_t15 158230 missing: 11 infinity: 0\n",
      "topic14_judiciary_t3 158230 missing: 5 infinity: 0\n",
      "topic14_judiciary_t15 158230 missing: 11 infinity: 0\n",
      "google_index_conflict_t2 158230 missing: 5 infinity: 0\n",
      "google_index_conflict_t14 158230 missing: 11 infinity: 0\n",
      "google_index_war_t2 158230 missing: 5 infinity: 0\n",
      "google_index_war_t14 158230 missing: 11 infinity: 0\n",
      "internet_t24 158230 missing: 32 infinity: 0\n",
      "internet_t36 158230 missing: 32 infinity: 0\n",
      "decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_5 158230 missing: 0 infinity: 0\n",
      "topic0_religion_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic1_politics_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic2_sanctions_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic3_life_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic4_energy_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic5_media_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic6_economics_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic7_health_t3_stock 158230 missing: 5 infinity: 0\n",
      "topic8_china_t3_stock 158230 missing: 5 infinity: 0\n",
      "media_google_internet\n",
      "ln_ged_sb_dep 158230 missing: 0 infinity: 0\n",
      "ln_ged_sb 158230 missing: 0 infinity: 0\n",
      "wdi_sp_pop_totl 158230 missing: 11 infinity: 0\n",
      "_wdi_sp_pop_totl 158230 missing: 11 infinity: 0\n",
      "google_index_conflict_t2 158230 missing: 5 infinity: 0\n",
      "google_index_conflict_t14 158230 missing: 11 infinity: 0\n",
      "google_index_war_t2 158230 missing: 5 infinity: 0\n",
      "google_index_war_t14 158230 missing: 11 infinity: 0\n",
      "internet_t24 158230 missing: 32 infinity: 0\n",
      "internet_t36 158230 missing: 32 infinity: 0\n",
      "decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "decay_ged_os_5 158230 missing: 0 infinity: 0\n",
      "google_index_conflict_t2_ra 158230 missing: 5 infinity: 0\n",
      "google_index_war_t2_ra 158230 missing: 5 infinity: 0\n",
      "internet_t24_ra 158230 missing: 32 infinity: 0\n",
      "splag_1_decay_ged_sb_5 158230 missing: 0 infinity: 0\n",
      "google_index_conflict_t2_ra_splag 158230 missing: 0 infinity: 0\n",
      "google_index_war_t2_ra_splag 158230 missing: 0 infinity: 0\n",
      "internet_t24_ra_splag 158230 missing: 0 infinity: 0\n"
     ]
    }
   ],
   "source": [
    "N=51\n",
    "for i in range(len(Datasets)):\n",
    "    df = Datasets[i]['df']\n",
    "    print(Datasets[i]['Name'])\n",
    "    for col in df.iloc[: , :N].columns:\n",
    "        print(col,len(df[col]), 'missing:', df[col].isnull().sum(), 'infinity:', np.isinf(df).values.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761eb9c",
   "metadata": {},
   "source": [
    "# Specify models in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425514d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fat_dev_mc_media_baseline_xgbrf media_baseline\n",
      "1 fat_dev_mc_media_topics_xgbrf media_topics\n",
      "2 fat_dev_mc_media_google_internet_hurdle media_google_internet\n",
      "3 fat_dev_mc_media_all_features_xgbrf media_all_features\n"
     ]
    }
   ],
   "source": [
    "from ModelDefinitions_media import DefineEnsembleModels\n",
    "\n",
    "ModelList = DefineEnsembleModels('cm')\n",
    "    \n",
    "for imodel,model in enumerate(ModelList):\n",
    "    print(imodel, model['modelname'], model['data_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b1b6322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelname': 'fat_dev_mc_media_baseline_xgbrf',\n",
       "  'algorithm': XGBRFRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bytree=None,\n",
       "                 early_stopping_rounds=None, enable_categorical=False,\n",
       "                 eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None, max_bin=None,\n",
       "                 max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "                 max_leaves=None, min_child_weight=None, missing=nan,\n",
       "                 monotone_constraints=None, n_estimators=300, n_jobs=4,\n",
       "                 num_parallel_tree=None, objective='reg:squarederror',\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 sampling_method=None, scale_pos_weight=None, ...),\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'media_baseline',\n",
       "  'queryset': 'fat_dev_mc_media_baseline',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Baseline model with a few conflict history features as well as log population, random forests regression model.',\n",
       "  'long_description': 'A very simple model with only five data columns (each column representing one feature): The number of fatalities in the same country at $t-1$, three decay functions of time since there was at least five fatalities in a single month, for each of the UCDP conflict types -- state-based, one-sided, or non-state conflict -- and log population size (Hegre2020RP,Pettersson2021JPR).The features in the baseline are included in all the models described below. This ensures that all models in the ensemble provides at least moderately good predictions, while guaranteeing diversity in feature sets and modelling approaches.',\n",
       "  'predstore_calib': 'cm_fat_dev_mc_media_baseline_xgbrf_calib',\n",
       "  'predstore_test': 'cm_fat_dev_mc_media_baseline_xgbrf_test'},\n",
       " {'modelname': 'fat_dev_mc_media_topics_xgbrf',\n",
       "  'algorithm': XGBRFRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bytree=None,\n",
       "                 early_stopping_rounds=None, enable_categorical=False,\n",
       "                 eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None, max_bin=None,\n",
       "                 max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "                 max_leaves=None, min_child_weight=None, missing=nan,\n",
       "                 monotone_constraints=None, n_estimators=300, n_jobs=4,\n",
       "                 num_parallel_tree=None, objective='reg:squarederror',\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 sampling_method=None, scale_pos_weight=None, ...),\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'media_topics',\n",
       "  'queryset': 'fat_dev_mc_media_topics',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Topics model by malika and chandler. data up to July 2022',\n",
       "  'long_description': 'A simple random forests regression model using topics data. includes topic shares time lagged to account for July update, one year lag from that adjusted value, running average and spatial lag.',\n",
       "  'predstore_calib': 'cm_fat_dev_mc_media_topics_xgbrf_calib',\n",
       "  'predstore_test': 'cm_fat_dev_mc_media_topics_xgbrf_test'},\n",
       " {'modelname': 'fat_dev_mc_media_google_internet_hurdle',\n",
       "  'algorithm': HurdleRegression(clf_name='XGBClassifier',\n",
       "                   clf_params={'n_estimators': 200, 'n_jobs': 4},\n",
       "                   reg_name='XGBRegressor',\n",
       "                   reg_params={'n_estimators': 200, 'n_jobs': 4}),\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'media_google_internet',\n",
       "  'queryset': 'fat_dev_mc_media_google_internet',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Google and internet model by malika and chandler. data up to August 2022',\n",
       "  'long_description': 'A hurdle regression model using google and internet data. Includes population data, internet per population appropriately lagged, and google internet indices appropriately lagged for conflict and war themes.',\n",
       "  'predstore_calib': 'cm_fat_dev_mc_media_google_internet_hurdle_calib',\n",
       "  'predstore_test': 'cm_fat_dev_mc_media_google_internet_hurdle_test'},\n",
       " {'modelname': 'fat_dev_mc_media_all_features_xgbrf',\n",
       "  'algorithm': XGBRFRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bytree=None,\n",
       "                 early_stopping_rounds=None, enable_categorical=False,\n",
       "                 eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None, max_bin=None,\n",
       "                 max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "                 max_leaves=None, min_child_weight=None, missing=nan,\n",
       "                 monotone_constraints=None, n_estimators=300, n_jobs=4,\n",
       "                 num_parallel_tree=None, objective='reg:squarederror',\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 sampling_method=None, scale_pos_weight=None, ...),\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'media_all_features',\n",
       "  'queryset': 'fat_dev_mc_media_all_features',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Google and internet model by malika and chandler. data up to August 2022',\n",
       "  'long_description': 'the everything model.',\n",
       "  'predstore_calib': 'cm_fat_dev_mc_media_all_features_xgbrf_calib',\n",
       "  'predstore_test': 'cm_fat_dev_mc_media_all_features_xgbrf_test'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613b4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fat_dev_mc_media_baseline_xgbrf media_baseline\n",
      "1 fat_dev_mc_media_topics_xgbrf media_topics\n",
      "2 fat_dev_mc_media_google_internet_hurdle media_google_internet\n",
      "3 fat_dev_mc_media_all_features_xgbrf media_all_features\n"
     ]
    }
   ],
   "source": [
    "document_ensemble(ModelList,'sb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c58f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fat_dev_mc_media_baseline_xgbrf\n",
      "Calibration partition 2022-11-02 12:12:27.285979\n",
      " * == Performing a run: \"fat_dev_mc_media_baseline_xgbrf_calib\" == * \n",
      "Model object named \"fat_dev_mc_media_baseline_xgbrf_calib\" with equivalent metadata already exists.\n",
      "Retrain is true, overwriting \"fat_dev_mc_media_baseline_xgbrf_calib\"\n",
      "Training model(s)...\n",
      "Storing \"fat_dev_mc_media_baseline_xgbrf_calib\"\n",
      "Trying to retrieve predictions 2022-11-02 12:13:29.347753\n",
      "pr_51_cm_fat_dev_mc_media_baseline_xgbrf_calib.parquet\n",
      "Test partition 2022-11-02 12:13:31.604549\n",
      " * == Performing a run: \"fat_dev_mc_media_baseline_xgbrf_test\" == * \n",
      "Model object named \"fat_dev_mc_media_baseline_xgbrf_test\" with equivalent metadata already exists.\n",
      "Retrain is true, overwriting \"fat_dev_mc_media_baseline_xgbrf_test\"\n",
      "Training model(s)...\n",
      "Storing \"fat_dev_mc_media_baseline_xgbrf_test\"\n",
      "Trying to retrieve predictions 2022-11-02 12:14:47.316254\n",
      "pr_51_cm_fat_dev_mc_media_baseline_xgbrf_test.parquet\n",
      "**************************************************************\n",
      "1 fat_dev_mc_media_topics_xgbrf\n",
      "Calibration partition 2022-11-02 12:14:49.699142\n",
      " * == Performing a run: \"fat_dev_mc_media_topics_xgbrf_calib\" == * \n",
      "Training model(s)...\n",
      "Storing \"fat_dev_mc_media_topics_xgbrf_calib\"\n",
      "Trying to retrieve predictions 2022-11-02 12:18:38.023379\n",
      "pr_51_cm_fat_dev_mc_media_topics_xgbrf_calib.parquet\n",
      "cm_fat_dev_mc_media_topics_xgbrf_calib , run fat_dev_mc_media does not exist, predicting\n",
      "Test partition 2022-11-02 12:18:46.148509\n",
      " * == Performing a run: \"fat_dev_mc_media_topics_xgbrf_test\" == * \n",
      "Training model(s)...\n",
      "Storing \"fat_dev_mc_media_topics_xgbrf_test\"\n",
      "Trying to retrieve predictions 2022-11-02 12:24:01.929632\n",
      "pr_51_cm_fat_dev_mc_media_topics_xgbrf_test.parquet\n",
      "cm_fat_dev_mc_media_topics_xgbrf_test , run fat_dev_mc_media does not exist, predicting\n",
      "**************************************************************\n",
      "2 fat_dev_mc_media_google_internet_hurdle\n",
      "Calibration partition 2022-11-02 12:24:09.923934\n",
      " * == Performing a run: \"fat_dev_mc_media_google_internet_hurdle_calib\" == * \n",
      "Training model(s)...\n",
      "Storing \"fat_dev_mc_media_google_internet_hurdle_calib\"\n",
      "Trying to retrieve predictions 2022-11-02 12:25:49.807893\n",
      "pr_51_cm_fat_dev_mc_media_google_internet_hurdle_calib.parquet\n",
      "cm_fat_dev_mc_media_google_internet_hurdle_calib , run fat_dev_mc_media does not exist, predicting\n",
      "Test partition 2022-11-02 12:25:57.453340\n",
      " * == Performing a run: \"fat_dev_mc_media_google_internet_hurdle_test\" == * \n",
      "Training model(s)...\n",
      "Storing \"fat_dev_mc_media_google_internet_hurdle_test\"\n",
      "Trying to retrieve predictions 2022-11-02 12:28:01.054068\n",
      "pr_51_cm_fat_dev_mc_media_google_internet_hurdle_test.parquet\n",
      "cm_fat_dev_mc_media_google_internet_hurdle_test , run fat_dev_mc_media does not exist, predicting\n",
      "**************************************************************\n",
      "3 fat_dev_mc_media_all_features_xgbrf\n",
      "Calibration partition 2022-11-02 12:28:08.882136\n",
      " * == Performing a run: \"fat_dev_mc_media_all_features_xgbrf_calib\" == * \n",
      "Training model(s)...\n",
      "Storing \"fat_dev_mc_media_all_features_xgbrf_calib\"\n",
      "Trying to retrieve predictions 2022-11-02 12:32:42.743385\n",
      "pr_51_cm_fat_dev_mc_media_all_features_xgbrf_calib.parquet\n",
      "cm_fat_dev_mc_media_all_features_xgbrf_calib , run fat_dev_mc_media does not exist, predicting\n",
      "Test partition 2022-11-02 12:32:50.778576\n",
      " * == Performing a run: \"fat_dev_mc_media_all_features_xgbrf_test\" == * \n",
      "Training model(s)...\n",
      "Storing \"fat_dev_mc_media_all_features_xgbrf_test\"\n",
      "Trying to retrieve predictions 2022-11-02 12:39:03.401419\n",
      "pr_51_cm_fat_dev_mc_media_all_features_xgbrf_test.parquet\n",
      "cm_fat_dev_mc_media_all_features_xgbrf_test , run fat_dev_mc_media does not exist, predicting\n",
      "**************************************************************\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "# Loop that checks whether the model exists, retrains if not, \n",
    "# and stores the predictions if they have not been stored before for this run.\n",
    "# To do: set the data_preprocessing to the function in the model dictionary\n",
    "\n",
    "level = 'cm'\n",
    "includeFuture = False\n",
    "\n",
    "from views_runs import Storage, StepshiftedModels\n",
    "from views_partitioning.data_partitioner import DataPartitioner\n",
    "from viewser import Queryset, Column\n",
    "from views_runs import operations\n",
    "from views_runs.run_result import RunResult\n",
    "\n",
    "i = 0\n",
    "for model in ModelList:\n",
    "    if model['algorithm'] != 'Rscript':\n",
    "        force_retrain = True\n",
    "        modelstore = storage.Storage()\n",
    "        ct = datetime.now()\n",
    "        print(i, model['modelname'])\n",
    "        print('Calibration partition', ct)\n",
    "        model['Algorithm_text'] = str(model['algorithm'])\n",
    "        model['RunResult_calib'] = RunResult.retrain_or_retrieve(\n",
    "                retrain            = force_retrain,\n",
    "                store              = modelstore,\n",
    "                partitioner        = DataPartitioner({\"calib\":calib_partitioner_dict}),\n",
    "                stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                queryset_name      = model['queryset'],\n",
    "                partition_name     = \"calib\",\n",
    "                timespan_name      = \"train\",\n",
    "                storage_name       = model['modelname'] + '_calib',\n",
    "                author_name        = \"HH\",\n",
    "        )\n",
    "\n",
    "    #    model['predstore_calib'] = level +  '_' + model['modelname'] + '_calib'\n",
    "        ct = datetime.now()\n",
    "        print('Trying to retrieve predictions', ct)\n",
    "        try:\n",
    "            predictions_calib = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_calib'])\n",
    "        except KeyError:\n",
    "            print(model['predstore_calib'], ', run',  run_id, 'does not exist, predicting')\n",
    "            predictions_calib = model['RunResult_calib'].run.predict(\"calib\",\"predict\", model['RunResult_calib'].data)\n",
    "            predictions_calib.forecasts.set_run(run_id)\n",
    "            predictions_calib.forecasts.to_store(name=model['predstore_calib'])\n",
    "\n",
    "        ct = datetime.now()\n",
    "        print('Test partition', ct)\n",
    "        modelstore = storage.Storage()\n",
    "        model['RunResult_test'] = RunResult.retrain_or_retrieve(\n",
    "                retrain            = force_retrain,\n",
    "                store              = modelstore,\n",
    "                partitioner        = DataPartitioner({\"test\":test_partitioner_dict}),\n",
    "                stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                queryset_name      = model['queryset'],\n",
    "                partition_name     = \"test\",\n",
    "                timespan_name      = \"train\",\n",
    "                storage_name       = model['modelname'] + '_test',\n",
    "                author_name        = \"HH\",\n",
    "        )\n",
    "        ct = datetime.now()\n",
    "        print('Trying to retrieve predictions', ct)\n",
    "    #    model['predstore_test'] = level +  '_' + model['modelname'] + '_test'\n",
    "        try:\n",
    "            predictions_test = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_test'])\n",
    "        except KeyError:\n",
    "            print(model['predstore_test'], ', run', run_id, 'does not exist, predicting')\n",
    "            predictions_test = model['RunResult_test'].run.predict(\"test\",\"predict\",model['RunResult_test'].data)\n",
    "            predictions_test.forecasts.set_run(run_id)\n",
    "            predictions_test.forecasts.to_store(name=model['predstore_test'])\n",
    "        # Predictions for true future\n",
    "        if includeFuture:\n",
    "            ct = datetime.now()\n",
    "            print('Future', ct)\n",
    "            modelstore = storage.Storage()\n",
    "            model['RunResult_future'] = RunResult.retrain_or_retrieve(\n",
    "                    retrain            = force_retrain,\n",
    "                    store              = modelstore,\n",
    "                    partitioner        = DataPartitioner({\"test\":future_partitioner_dict}),\n",
    "                    stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "                    dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "                    queryset_name      = model['queryset'],\n",
    "                    partition_name     = \"test\",\n",
    "                    timespan_name      = \"train\",\n",
    "                    storage_name       = model['modelname'] + '_future',\n",
    "                    author_name        = \"HH\",\n",
    "            )\n",
    "            ct = datetime.now()\n",
    "            print('Trying to retrieve predictions', ct)\n",
    "            model['predstore_future'] = level +  '_' + model['modelname'] + '_f' + str(FutureStart)\n",
    "            try:\n",
    "                predictions_future = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstore_future'])\n",
    "            except KeyError:\n",
    "                print(model['predstore_future'], ', run', run_id, 'does not exist, predicting')\n",
    "                predictions_future = model['RunResult_future'].run.future_point_predict(FutureStart,model['RunResult_future'].data)\n",
    "                predictions_future.forecasts.set_run(run_id)\n",
    "                predictions_future.forecasts.to_store(name=model['predstore_future'])  \n",
    "        print('**************************************************************')\n",
    "    i = i + 1\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a9bb08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ln_ged_sb_dep</th>\n",
       "      <th>ln_ged_sb</th>\n",
       "      <th>wdi_sp_pop_totl</th>\n",
       "      <th>_wdi_sp_pop_totl</th>\n",
       "      <th>topic0_religion_t3</th>\n",
       "      <th>topic0_religion_t15</th>\n",
       "      <th>topic1_politics_t3</th>\n",
       "      <th>topic1_politics_t15</th>\n",
       "      <th>topic2_sanctions_3</th>\n",
       "      <th>topic2_sanctions_t15</th>\n",
       "      <th>...</th>\n",
       "      <th>step_pred_33</th>\n",
       "      <th>step_pred_34</th>\n",
       "      <th>step_pred_35</th>\n",
       "      <th>step_pred_36</th>\n",
       "      <th>step_pred_4</th>\n",
       "      <th>step_pred_5</th>\n",
       "      <th>step_pred_6</th>\n",
       "      <th>step_pred_7</th>\n",
       "      <th>step_pred_8</th>\n",
       "      <th>step_pred_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>3.218876</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>12.752180</td>\n",
       "      <td>12.650400</td>\n",
       "      <td>2.13735</td>\n",
       "      <td>6.15594</td>\n",
       "      <td>0.10543</td>\n",
       "      <td>0.08998</td>\n",
       "      <td>...</td>\n",
       "      <td>2.532329</td>\n",
       "      <td>2.659568</td>\n",
       "      <td>2.357273</td>\n",
       "      <td>2.248837</td>\n",
       "      <td>1.768521</td>\n",
       "      <td>1.430680</td>\n",
       "      <td>1.381671</td>\n",
       "      <td>1.878586</td>\n",
       "      <td>1.588895</td>\n",
       "      <td>2.026045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>15.161450</td>\n",
       "      <td>14.229150</td>\n",
       "      <td>3.20609</td>\n",
       "      <td>4.83517</td>\n",
       "      <td>0.09004</td>\n",
       "      <td>0.09765</td>\n",
       "      <td>...</td>\n",
       "      <td>2.740053</td>\n",
       "      <td>2.668735</td>\n",
       "      <td>2.059299</td>\n",
       "      <td>2.554636</td>\n",
       "      <td>2.485806</td>\n",
       "      <td>1.304931</td>\n",
       "      <td>1.503598</td>\n",
       "      <td>1.312958</td>\n",
       "      <td>1.721088</td>\n",
       "      <td>1.863862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>21.077900</td>\n",
       "      <td>13.706790</td>\n",
       "      <td>4.36394</td>\n",
       "      <td>3.63354</td>\n",
       "      <td>0.18347</td>\n",
       "      <td>0.11137</td>\n",
       "      <td>...</td>\n",
       "      <td>2.474459</td>\n",
       "      <td>2.767469</td>\n",
       "      <td>2.082304</td>\n",
       "      <td>2.309595</td>\n",
       "      <td>1.694217</td>\n",
       "      <td>2.498828</td>\n",
       "      <td>1.480119</td>\n",
       "      <td>1.355356</td>\n",
       "      <td>1.364338</td>\n",
       "      <td>1.879686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>17.211529</td>\n",
       "      <td>13.411079</td>\n",
       "      <td>3.14073</td>\n",
       "      <td>4.43855</td>\n",
       "      <td>0.15888</td>\n",
       "      <td>0.06939</td>\n",
       "      <td>...</td>\n",
       "      <td>2.897392</td>\n",
       "      <td>2.581539</td>\n",
       "      <td>2.242971</td>\n",
       "      <td>2.358017</td>\n",
       "      <td>2.403450</td>\n",
       "      <td>1.620644</td>\n",
       "      <td>2.632906</td>\n",
       "      <td>1.425801</td>\n",
       "      <td>1.264489</td>\n",
       "      <td>1.669755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>11062113.0</td>\n",
       "      <td>14.946149</td>\n",
       "      <td>17.431931</td>\n",
       "      <td>2.81162</td>\n",
       "      <td>2.98259</td>\n",
       "      <td>0.06183</td>\n",
       "      <td>0.26696</td>\n",
       "      <td>...</td>\n",
       "      <td>2.849369</td>\n",
       "      <td>2.834021</td>\n",
       "      <td>1.997118</td>\n",
       "      <td>2.378682</td>\n",
       "      <td>2.567327</td>\n",
       "      <td>2.104240</td>\n",
       "      <td>1.634651</td>\n",
       "      <td>2.268459</td>\n",
       "      <td>1.117458</td>\n",
       "      <td>1.397199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ln_ged_sb_dep  ln_ged_sb  wdi_sp_pop_totl  _wdi_sp_pop_totl  \\\n",
       "month_id                                                                \n",
       "488            3.218876   3.218876       11062113.0        11062113.0   \n",
       "489            0.693147   0.693147       11062113.0        11062113.0   \n",
       "490            1.609438   1.609438       11062113.0        11062113.0   \n",
       "491            2.484907   2.484907       11062113.0        11062113.0   \n",
       "492            0.000000   0.000000       11062113.0        11062113.0   \n",
       "\n",
       "          topic0_religion_t3  topic0_religion_t15  topic1_politics_t3  \\\n",
       "month_id                                                                \n",
       "488                12.752180            12.650400             2.13735   \n",
       "489                15.161450            14.229150             3.20609   \n",
       "490                21.077900            13.706790             4.36394   \n",
       "491                17.211529            13.411079             3.14073   \n",
       "492                14.946149            17.431931             2.81162   \n",
       "\n",
       "          topic1_politics_t15  topic2_sanctions_3  topic2_sanctions_t15  ...  \\\n",
       "month_id                                                                 ...   \n",
       "488                   6.15594             0.10543               0.08998  ...   \n",
       "489                   4.83517             0.09004               0.09765  ...   \n",
       "490                   3.63354             0.18347               0.11137  ...   \n",
       "491                   4.43855             0.15888               0.06939  ...   \n",
       "492                   2.98259             0.06183               0.26696  ...   \n",
       "\n",
       "          step_pred_33  step_pred_34  step_pred_35  step_pred_36  step_pred_4  \\\n",
       "month_id                                                                        \n",
       "488           2.532329      2.659568      2.357273      2.248837     1.768521   \n",
       "489           2.740053      2.668735      2.059299      2.554636     2.485806   \n",
       "490           2.474459      2.767469      2.082304      2.309595     1.694217   \n",
       "491           2.897392      2.581539      2.242971      2.358017     2.403450   \n",
       "492           2.849369      2.834021      1.997118      2.378682     2.567327   \n",
       "\n",
       "          step_pred_5  step_pred_6  step_pred_7  step_pred_8  step_pred_9  \n",
       "month_id                                                                   \n",
       "488          1.430680     1.381671     1.878586     1.588895     2.026045  \n",
       "489          1.304931     1.503598     1.312958     1.721088     1.863862  \n",
       "490          2.498828     1.480119     1.355356     1.364338     1.879686  \n",
       "491          1.620644     2.632906     1.425801     1.264489     1.669755  \n",
       "492          2.104240     1.634651     2.268459     1.117458     1.397199  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the future predictions\n",
    "\n",
    "\n",
    "predictions_test.xs(246,level=1).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b52249",
   "metadata": {},
   "source": [
    "## Notes on training time for the various algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are calculated in minutes for the hh20 feature set (with about 40 features), for all 36 steps, calibration (c) and test (t) partitions, also include generating predictions, and are approximate:\n",
    "\n",
    "#nj=12 (number of threads)\n",
    "#scikit random forest:        21:13 (c), 26:20 (t) RandomForestRegressor(n_estimators=200, n_jobs=nj)\n",
    "#XGB random forest:           06:02 (c), 07:51 (t) XGBRFRegressor(n_estimators=300,n_jobs=nj)\n",
    "#scikit gbm:                  13:59 (c), 15:55 (t) GradientBoostingRegressor(), \n",
    "#scikit hurdle random forest: 07:32 (c), 09:49 (t) For both clf and reg: (n_estimators=200, n_jobs=nj)\n",
    "#XGB hurdle xgb:              01:26 (c), 01:32 (t) For both clf and reg:                n_estimators=200,tree_method='hist',n_jobs=nj)\n",
    "#scikit histgbm:              01:17 (c), 01:20 (t) HistGradientBoostingRegressor(max_iter=200)\n",
    "#XGB xgb:                     01:00 (c), 01:04 (t) XGBRegressor(n_estimators=200,tree_method='hist',n_jobs=nj)\n",
    "#lightgbm gbm:                00:25 (c), --    (t) LGBMRegressor(n_estimators=100,num_threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71483a35",
   "metadata": {},
   "source": [
    "# Various helper functions and tools...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81f053fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "views-forecasts           0.5.3                    pypi_0    pypi\r\n"
     ]
    }
   ],
   "source": [
    "!conda list | grep views-forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7570c6",
   "metadata": {},
   "source": [
    "# Retrieving external forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b30211fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve David's Markov models\n",
    "# To do: rewrite the model dictionary to the new, slimmer version.\n",
    "DRList = []\n",
    "\n",
    "\n",
    "model = {\n",
    "    'modelname':   'fat_hh20_Markov_glm',\n",
    "    'algorithm': [],\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':      'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "}\n",
    "DRList.append(model)\n",
    "\n",
    "model = {\n",
    "    'modelname':   'fat_hh20_Markov_rf',\n",
    "    'algorithm': [],\n",
    "    'depvar': \"ln_ged_sb_dep\",\n",
    "    'data_train':      'hh20',\n",
    "    'queryset': 'hh_20_features',\n",
    "}\n",
    "\n",
    "DRList.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8dc1acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelname': 'fat_hh20_Markov_glm',\n",
       "  'algorithm': [],\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'hh20',\n",
       "  'queryset': 'hh_20_features',\n",
       "  'predictions_file_calib': '/Users/malika/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/vmm_glm_hh20_0125_alt_calib.csv',\n",
       "  'predictions_file_test': '/Users/malika/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/vmm_glm_hh20_0125_alt_test.csv',\n",
       "  'predictions_file_future': '/Users/malika/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/vmm_glm_hh20_506.csv'},\n",
       " {'modelname': 'fat_hh20_Markov_rf',\n",
       "  'algorithm': [],\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'hh20',\n",
       "  'queryset': 'hh_20_features',\n",
       "  'predictions_file_calib': '/Users/malika/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/vmm_rf_hh20_0125_alt_calib.csv',\n",
       "  'predictions_file_test': '/Users/malika/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/vmm_rf_hh20_0125_alt_test.csv',\n",
       "  'predictions_file_future': '/Users/malika/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/vmm_rf_hh20_505.csv'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DRList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41de188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'/Users/{os.getlogin()}/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "\n",
    "DRList[0]['predictions_file_calib'] = path + 'vmm_glm_hh20_0125_alt_calib.csv'\n",
    "DRList[0]['predictions_file_test'] = path + 'vmm_glm_hh20_0125_alt_test.csv'\n",
    "DRList[0]['predictions_file_future'] = path + 'vmm_glm_hh20_506.csv'\n",
    "\n",
    "DRList[1]['predictions_file_calib'] = path + 'vmm_rf_hh20_0125_alt_calib.csv'\n",
    "DRList[1]['predictions_file_test'] = path + 'vmm_rf_hh20_0125_alt_test.csv'\n",
    "DRList[1]['predictions_file_future'] = path + 'vmm_rf_hh20_505.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7162915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/malika/Dropbox (ViEWS)/ViEWS/Projects/PredictingFatalities/Predictions/cm/preds/\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cea1e5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fat_dev_mc_media_baseline_xgbrf\n",
      "fat_dev_mc_media_topics_xgbrf\n",
      "fat_dev_mc_media_google_internet_hurdle\n",
      "fat_dev_mc_media_all_features_xgbrf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model in ModelList:\n",
    "    print(model['modelname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86478962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_51_cm_fat_dev_mc_media_baseline_xgbrf_calib.parquet\n",
      "pr_51_cm_fat_dev_mc_media_baseline_xgbrf_test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Storing Markov models in central storage\n",
    "# Retrieving dependent variable\n",
    "target_calib = pd.DataFrame.forecasts.read_store('cm_fat_dev_mc_media_baseline_xgbrf_calib', run=run_id)['ln_ged_sb_dep']\n",
    "target_test = pd.DataFrame.forecasts.read_store('cm_fat_dev_mc_media_baseline_xgbrf_test', run=run_id)['ln_ged_sb_dep']\n",
    "level = 'cm'\n",
    "for model in DRList:\n",
    "    df_calib = pd.read_csv(model['predictions_file_calib'],index_col=['month_id','country_id'])\n",
    "    df_test = pd.read_csv(model['predictions_file_test'],index_col=['month_id','country_id'])\n",
    "    df_future = pd.read_csv(model['predictions_file_future'],index_col=['month_id','country_id'])\n",
    "    df_calib['ln_ged_sb_dep'] = target_calib\n",
    "    df_test['ln_ged_sb_dep'] = target_test\n",
    "    df_future['ln_ged_sb_dep'] = np.nan # Empty dependent variable column for consistency/required by prediction storage function\n",
    "    stored_modelname = level + '_' + model['modelname'] + '_calib'\n",
    "    df_calib.forecasts.set_run(run_id)\n",
    "    df_calib.forecasts.to_store(name=stored_modelname, overwrite=True)\n",
    "    stored_modelname = level + '_' + model['modelname'] + '_test'\n",
    "    df_test.forecasts.set_run(run_id)\n",
    "    df_test.forecasts.to_store(name=stored_modelname, overwrite=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caf8be93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelname': 'fat_dev_mc_media_baseline_xgbrf',\n",
       "  'algorithm': XGBRFRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bytree=None,\n",
       "                 early_stopping_rounds=None, enable_categorical=False,\n",
       "                 eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None, max_bin=None,\n",
       "                 max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "                 max_leaves=None, min_child_weight=None, missing=nan,\n",
       "                 monotone_constraints=None, n_estimators=300, n_jobs=4,\n",
       "                 num_parallel_tree=None, objective='reg:squarederror',\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 sampling_method=None, scale_pos_weight=None, ...),\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'media_baseline',\n",
       "  'queryset': 'fat_dev_mc_media_baseline',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Baseline model with a few conflict history features as well as log population, random forests regression model.',\n",
       "  'long_description': 'A very simple model with only five data columns (each column representing one feature): The number of fatalities in the same country at $t-1$, three decay functions of time since there was at least five fatalities in a single month, for each of the UCDP conflict types -- state-based, one-sided, or non-state conflict -- and log population size (Hegre2020RP,Pettersson2021JPR).The features in the baseline are included in all the models described below. This ensures that all models in the ensemble provides at least moderately good predictions, while guaranteeing diversity in feature sets and modelling approaches.',\n",
       "  'predstore_calib': 'cm_fat_dev_mc_media_baseline_xgbrf_calib',\n",
       "  'predstore_test': 'cm_fat_dev_mc_media_baseline_xgbrf_test',\n",
       "  'Algorithm_text': \"XGBRFRegressor(base_score=None, booster=None, callbacks=None,\\n               colsample_bylevel=None, colsample_bytree=None,\\n               early_stopping_rounds=None, enable_categorical=False,\\n               eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\\n               importance_type=None, interaction_constraints=None, max_bin=None,\\n               max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\\n               max_leaves=None, min_child_weight=None, missing=nan,\\n               monotone_constraints=None, n_estimators=300, n_jobs=4,\\n               num_parallel_tree=None, objective='reg:squarederror',\\n               predictor=None, random_state=None, reg_alpha=None,\\n               sampling_method=None, scale_pos_weight=None, ...)\",\n",
       "  'RunResult_calib': RunResult(training_date = 2022-11-01),\n",
       "  'RunResult_test': RunResult(training_date = 2022-11-01)},\n",
       " {'modelname': 'fat_dev_mc_media_topics_xgbrf',\n",
       "  'algorithm': XGBRFRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bytree=None,\n",
       "                 early_stopping_rounds=None, enable_categorical=False,\n",
       "                 eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None, max_bin=None,\n",
       "                 max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "                 max_leaves=None, min_child_weight=None, missing=nan,\n",
       "                 monotone_constraints=None, n_estimators=300, n_jobs=4,\n",
       "                 num_parallel_tree=None, objective='reg:squarederror',\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 sampling_method=None, scale_pos_weight=None, ...),\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'media_topics',\n",
       "  'queryset': 'fat_dev_mc_media_topics',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Topics model by malika and chandler. data up to July 2022',\n",
       "  'long_description': 'A simple random forests regression model using topics data. includes topic shares time lagged to account for July update, one year lag from that adjusted value, running average and spatial lag.',\n",
       "  'predstore_calib': 'cm_fat_dev_mc_media_topics_xgbrf_calib',\n",
       "  'predstore_test': 'cm_fat_dev_mc_media_topics_xgbrf_test',\n",
       "  'Algorithm_text': \"XGBRFRegressor(base_score=None, booster=None, callbacks=None,\\n               colsample_bylevel=None, colsample_bytree=None,\\n               early_stopping_rounds=None, enable_categorical=False,\\n               eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\\n               importance_type=None, interaction_constraints=None, max_bin=None,\\n               max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\\n               max_leaves=None, min_child_weight=None, missing=nan,\\n               monotone_constraints=None, n_estimators=300, n_jobs=4,\\n               num_parallel_tree=None, objective='reg:squarederror',\\n               predictor=None, random_state=None, reg_alpha=None,\\n               sampling_method=None, scale_pos_weight=None, ...)\",\n",
       "  'RunResult_calib': RunResult(training_date = 2022-11-02),\n",
       "  'RunResult_test': RunResult(training_date = 2022-11-02)},\n",
       " {'modelname': 'fat_dev_mc_media_google_internet_hurdle',\n",
       "  'algorithm': HurdleRegression(clf_name='XGBClassifier',\n",
       "                   clf_params={'n_estimators': 200, 'n_jobs': 4},\n",
       "                   reg_name='XGBRegressor',\n",
       "                   reg_params={'n_estimators': 200, 'n_jobs': 4}),\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'media_google_internet',\n",
       "  'queryset': 'fat_dev_mc_media_google_internet',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Google and internet model by malika and chandler. data up to August 2022',\n",
       "  'long_description': 'A hurdle regression model using google and internet data. Includes population data, internet per population appropriately lagged, and google internet indices appropriately lagged for conflict and war themes.',\n",
       "  'predstore_calib': 'cm_fat_dev_mc_media_google_internet_hurdle_calib',\n",
       "  'predstore_test': 'cm_fat_dev_mc_media_google_internet_hurdle_test',\n",
       "  'Algorithm_text': \"HurdleRegression(clf_name='XGBClassifier',\\n                 clf_params={'n_estimators': 200, 'n_jobs': 4},\\n                 reg_name='XGBRegressor',\\n                 reg_params={'n_estimators': 200, 'n_jobs': 4})\",\n",
       "  'RunResult_calib': RunResult(training_date = 2022-11-02),\n",
       "  'RunResult_test': RunResult(training_date = 2022-11-02)},\n",
       " {'modelname': 'fat_dev_mc_media_all_features_xgbrf',\n",
       "  'algorithm': XGBRFRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                 colsample_bylevel=None, colsample_bytree=None,\n",
       "                 early_stopping_rounds=None, enable_categorical=False,\n",
       "                 eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\n",
       "                 importance_type=None, interaction_constraints=None, max_bin=None,\n",
       "                 max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "                 max_leaves=None, min_child_weight=None, missing=nan,\n",
       "                 monotone_constraints=None, n_estimators=300, n_jobs=4,\n",
       "                 num_parallel_tree=None, objective='reg:squarederror',\n",
       "                 predictor=None, random_state=None, reg_alpha=None,\n",
       "                 sampling_method=None, scale_pos_weight=None, ...),\n",
       "  'depvar': 'ln_ged_sb_dep',\n",
       "  'data_train': 'media_all_features',\n",
       "  'queryset': 'fat_dev_mc_media_all_features',\n",
       "  'preprocessing': 'float_it',\n",
       "  'level': 'cm',\n",
       "  'description': 'Google and internet model by malika and chandler. data up to August 2022',\n",
       "  'long_description': 'the everything model.',\n",
       "  'predstore_calib': 'cm_fat_dev_mc_media_all_features_xgbrf_calib',\n",
       "  'predstore_test': 'cm_fat_dev_mc_media_all_features_xgbrf_test',\n",
       "  'Algorithm_text': \"XGBRFRegressor(base_score=None, booster=None, callbacks=None,\\n               colsample_bylevel=None, colsample_bytree=None,\\n               early_stopping_rounds=None, enable_categorical=False,\\n               eval_metric=None, gamma=None, gpu_id=None, grow_policy=None,\\n               importance_type=None, interaction_constraints=None, max_bin=None,\\n               max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\\n               max_leaves=None, min_child_weight=None, missing=nan,\\n               monotone_constraints=None, n_estimators=300, n_jobs=4,\\n               num_parallel_tree=None, objective='reg:squarederror',\\n               predictor=None, random_state=None, reg_alpha=None,\\n               sampling_method=None, scale_pos_weight=None, ...)\",\n",
       "  'RunResult_calib': RunResult(training_date = 2022-11-02),\n",
       "  'RunResult_test': RunResult(training_date = 2022-11-02)}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce44d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
